<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en">

<head>

    <meta charset="utf-8">
    <title> MAI 2022 Workshop </title>
    <link rel="stylesheet" href="style_challenge.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,700italic,400italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i" rel="stylesheet"> 
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <script src="js/modernizr.js"></script>
    <meta name="viewport" content="width=device-width">
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-118781498-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-118781498-1');
	  
	</script>
    
</head>

<body data-spy="scroll" data-offset="0" data-target="#navbar-main">

<nav>
	<ul class="left_bar">
		<a href="index.html"> <img src="assets/img/splash.png"> </a>
	</ul>
	<ul class="middle_bar">
		<a href="#live"> <li style="color: rgb(238, 0, 106);"> LIVE <svg style="height: 0.6vw; width: 0.6vw; margin-left: 0.4vw;" class="blinking"><circle cx="0.3vw" cy="0.3vw" r="0.3vw" fill="rgb(238, 0, 106)" /></svg>
</li></a>
		<a href="#schedule"> <li> SCHEDULE </li></a>
		<a href="#challenges"> <li> CHALLENGES </li></a>
		<a href="#runtime"> <li> RUNTIME </li></a>
		<a href="#organizers"> <li> ORGANIZERS </li></a>
		<a href="#dates"> <li> DATES </li></a>
		<a href="#papers"> <li> PAPERS </li></a>
		<!--a href="#tutorial"> <li> TUTORIAL </li></a-->
		<a href="#contacts"> <li> CONTACTS </li></a>
	</ul>
	<ul class="right_bar">
		<a href="#submission" class="submit_button"> <li> SUBMIT </li></a>
	</ul>
</nav>

<img src="assets/img/mai_logo_2022.jpg" style="width: 100%; margin-top: 4.0vw;">

<div id="workshop">

		<div class="description"> 	
		
		<p> Over the past years, mobile AI-based applications are becoming more and more ubiquitous. Various deep learning models can now be found on any mobile device, starting from smartphones running portrait segmentation, image enhancement, face recognition and natural language processing models, to smart-TV boards coming with sophisticated image super-resolution algorithms. The performance of mobile NPUs and DSPs is also increasing dramatically, making it possible to run complex deep learning models and to achieve fast runtime in the majority of tasks.
		</p>

		<p style="margin-top: 1.2vw;"> While many research works targeted at efficient deep learning models have been proposed recently, the evaluation of the obtained solutions is usually happening on desktop CPUs and GPUs, making it nearly impossible to estimate the actual inference time and memory consumption on real mobile hardware. To address this problem, we introduce the first Mobile AI Workshop, where all deep learning solutions are developed for and evaluated on mobile devices.
		</p>
		
		<p style="margin-top: 1.2vw;"> Due to the performance of the last-generation mobile AI hardware, the topics considered in this workshop will go beyond the simple classification tasks, and will include such challenging problems as <span style="font-style: italic;">image denoising, HDR photography, accurate depth estimation, learned image ISP pipeline, real-time image and video super-resolution</span>. All information about the challenges, papers, invited talks and workshop industry partners is provided below.
		</p>
	</div>
<div>


<div id="live">
	<p class="title"> LIVE </p>	
	<p class="tba">[ TO BE ANNOUNCED ]</p>
	<!--iframe style="width: 50vw; height: 28.125vw;" src="https://youtu.be/tdpmvy2Cab0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe-->
	
	<!--<iframe style="width: 50vw; height: 28.125vw;" src="https://www.youtube.com/embed/tdpmvy2Cab0?start=795" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>></iframe>-->
	
	<!--p style="margin-top: 2.4vw; font-size: 0.98vw;"> Join the main workshop Zoom conference: <a href="https://ethz.zoom.us/j/67422434389" target="_blank" style="color: rgb(234, 29, 111); text-decoration-style: dotted;">https://ethz.zoom.us/j/67422434389</a></p-->
	
	<p style="margin-top: 2.4vw; font-size: 0.98vw;"> Have some questions to the speakers?&nbsp; Leave them on the <a href="https://ai-benchmark.net/index.php?threads/live-mai-workshop-leave-your-questions-to-the-speakers-in-this-thread.32/" target="_blank" style="color: rgba(23, 142, 210, 1.0); text-decoration-style: dotted;">AI Benchmark Forum ⇢</a></p>	
	
</div>

<div id="schedule">
	<p class="title"> SCHEDULE </p>	
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-andrey").style.display="inline-block";' src="assets/speakers/andrey.jpg">
			<a href="https://vision.ee.ethz.ch/" target="_blank"><img style="margin-top: 0.7vw; width: 75%" src="assets/organizers/eth-zurich.png"></a>
			<!--a href="https://ai-benchmark.com/" target="_blank"><img style="margin-top: 0.7vw; width: 100%" src="assets/organizers/ai_witchlabs.png"></a-->
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;  line-height: 1.4vw;">Deep Learning on Mobile Devices:&nbsp; <span style="font-weight: 500;">What's New in 2022?</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">08:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-andrey").style.display="inline-block";'>Andrey Ignatov</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">AI Benchmark Project Lead, ETH Zurich</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-andrey"> <span style="font-weight: 600;">Biography:</span> Andrey Ignatov is the project lead of the AI Benchmark initiative targeted at performance evaluation of mobile, IoT and desktop hardware at ETH Zurich, Switzerland. His PhD research was aimed at designing efficient image processing models for smartphone NPUs / DSPs and developing the next-generation deep learning based mobile camera ISP solution. He is the lecturer on Deep Learning for Smartphones course at ETH Zurich and the main author of the AI Benchmark papers describing the current state of deep learning and AI hardware acceleration on mobile devices. He is a co-founder of the AI Witchlabs and co-organizer of the NTIRE and AIM events. His main line of research is focused on image restoration and automatic image quality enhancement, adaptation of AI applications for mobile devices and benchmarking machine learning hardware.</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> In this tutorial, we will first recall all basic concepts, steps and optimizations required for efficient AI inference on mobile devices. Next, we will go into more detail about the latest mobile platforms from Qualcomm, MediaTek, Google, Samsung, Unisoc and Apple released during the past year, and will compare their performance in real-world computer vision AI tasks. We will also review the recent Android AI software stack updates, and will compare the deployment of TensorFlow Lite models on Android and iOS. Finally, we will talk about the power efficiency of mobile chipsets and their NPUs, and will analyze energy consumption of all popular Android SoCs.</p>
		</div>

	</div>
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer; margin-bottom: 0.4vw;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Dr_Allen_Lu.jpg">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Cheng_Ming_Chiang.jpg">
			<a href="https://www.mediatek.com/" target="_blank"><img style="margin-top: 0.7vw; width: 80%" src="assets/organizers/mediatek.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;">Mobile AI Trend and The Power Performance Metric</p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">09:15 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";'>Dr. Allen Lu</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior Director, Computing and AI Technology Group at MediaTek</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">09:45 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";'>Cheng-Ming Chiang</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Technical Manager, MediaTek Inc.</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-allen"> <span style="font-weight: 600;">Biography:</span> Dr. Lu is the Senior Director of Computing and Artificial intelligence Technology Group at MediaTek. He is responsible for the ML/DL algorithm/tool development for mobile phone, camera, tablet, and TV products.  Prior to MediaTek, Dr. Lu served as the General Manager of Video IoT (iVoT) Business Unit at Novatek. He was responsible for business and technology planning and execution.  Prior to Novatek, Dr. Lu funded Afatek, a fabless company developing RF, digital modulator, and demodulator chips.  Afatek was acquired by iTE in 2008.</br></br>

Dr. Lu worked in Silicon Valley from 1997-2002. He joined the Excess Bandwidth Corp. specializing in signal processing.  Excess Bandwidth Corp. was acquired by Virata Corp (now Conexant) in 2000.  Dr. Lu was a member of technical staff at Hewlett Packard prior to Excess Bandwidth.</br></br>

Dr. Lu received his M.S. and Ph.D. in Electrical Engineering from Stanford University.  He published more than 30 papers in the optical communication and photonic switching fields.  
</br></br>

Cheng-Ming Chiang is the Technical Manager of Computing and Artificial intelligence Technology Group at MediaTek. He acts as an AI algorithm architect and is responsible for Algorithm-Software-Hardware co-design for AI accelerator. He co-defines next generation AI accelerator with hardware architects from algorithm’s point of view. He also guides customers and ecosystem to deploy their deep learning models on MediaTek APU.</br></br>

Cheng-Ming Chiang is the program committee of NTIRE and AIM Workshops. He also co-organizes challenges in Mobile AI Workshops.
He received both his M.S. and B.S. in Electronic Engineering from National Chiao Tung University in Taiwan in 2013 and 2011, respectively.
</br></br>
</p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Mobile AI is evolving from short-burst applications like photos to sustained ones such as videos, games, and VR/MR.  The model complexity increases dramatically over time.  To enable new mobile AI applications, a combination of process advances, hardware architecture improvements, and computation complexity reduction is required.  Running bigger models with higher-resolution data under a limited power budget is the most critical challenge for mobile AI applications.

The current performance metric based on the minimum inference time, peak performance metric, is a good measurement for short-burst AI applications, but not the appropriate metric for sustained AI applications because it does not consider the most critical power budget constraint.

We suggest a power-performance curve (power curve) to derive the sustained performance metric at a fixed power budget and to address the mobile AI’s trend from peak performance to sustained performance.   We will set up a video super resolution challenge to promote the sustained performance metric.
 </p>
		</div>

	</div>

	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-felix").style.display="inline-block";' src="assets/speakers/Felix_Baum.jpg">
			<a href="https://www.qualcomm.com/" target="_blank"><img style="margin-top: 0.7vw; width: 94%;" src="assets/organizers/qc_logo_flt_rgb_blu_pos.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">A Practical Guide to Getting the DNN Accuracy You Need and the Performance You Deserve </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">10:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-felix").style.display="inline-block";'>Felix Baum</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Director Product Management, Qualcomm Technologies</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-felix"> <span style="font-weight: 600;">Biography:</span> Felix Baum is responsible for AI Software Products at Qualcomm Technologies Inc. (QTI). Felix has spent 20+ years in the embedded industry, both as an embedded developer and as a product manager. He previously led QTI product management for the Hexagon Software supporting DSPs with scalar, vector and tensor accelerators for camera, video, machine learning and audio verticals. Prior to that, he led marketing and product management efforts for various real-time operating system technologies. His career began at NASA's Jet Propulsion Laboratory at the California Institute of Technology, designing flight software for various spacecraft and managing integration for a launch campaign of the GRACE mission. Felix holds a master's degree in computer science from the California State University at Northridge and a Master of Business Administration from the University of California at Los Angeles.</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Every day, developers struggle to take DNN workloads that were originally developed on workstations and migrate them to run on edge devices. Whether the application is in mobile, compute, IoT, XR or automotive, most AI developers start their algorithm development in the cloud or on a workstation and later migrate to on-device as an afterthought. Qualcomm is helping these developers on multiple fronts-democratizing AI at the edge by supporting frameworks and data types that developers are most familiar with, and at the same time building a set of tools to assist sophisticated developers who are taking extra steps to extract the best performance and power efficiency. In this session, we present the workflow and steps for effectively migrating DNN workloads to the edge. We discuss quantization issues, explore how the accuracy of models affects performance and power and outline the Qualcomm tools that help developers successfully launch new use cases on mobile and other edge devices.</p>
		</div>

	</div>

	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-gilberto").style.display="inline-block";' src="">
			<a href="https://www.intel.com/content/www/us/en/homepage.html" target="_blank"><img style="margin-top: 0.7vw; width: 70%;" src="assets/organizers/intel-footer-logo.svg"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;">Optimizing TensorFlow on Intel® Core platforms</p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">10:45 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Vivek Kumar</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior AI Software Architect at Intel</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">11:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Manuj Sabharwal</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior AI Software Engineer at Intel</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">11:15 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Ramesh AG</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Principal Engineer at Intel</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-empty"> <span style="font-weight: 600;">Biography:</span> Expert in embedded solutions for telecommunications, video processing and artificial intelligence. Co-founded start-ups and lead teams of more than 70 engineers distributed worldwide. 20 years of experience leading global teams and organizations, Executive MBA from London Business School and MEng in Telecommunications.</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Intel optimization with TensorFlow on Hybrid architecture Intel is
collaborating with Google for TensorFlow optimizations enabled via Intel oneDNN to accelerate key performance-intensive operations such as convolution, matrix multiplication, and batch normalization using the AI acceleration instructions Intel(R) AVX2 and Intel(R) DL Boost.   Intel oneDNN is an open-source, cross-platform performance library for DL applications.   Additional Intel optimizations include aggressive op fusions for optimal cache performance.  In this tutorial, we will show basic concepts, steps and optimizations required to improve performance on Intel Hybrid core processor AlderLake.  This session also covers quantization techniques and tools to boost model inference performance using low precision, sparsity and quantization to leverage the 8-bit acceleration instructions on Intel CPUs. </p>
		</div>

	</div>
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-abdel").style.display="inline-block";' src="assets/speakers/abdel.jpg">
			<a href="https://www.synaptics.com/" target="_blank"><img style="margin-top: 0.7vw; width: 90%;" src="assets/organizers/synaptics-rebrand-logo.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">AI on the Edge @Synaptics : HW and SW Products and Development </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">11:30 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-abdel").style.display="inline-block";'>Abdel Younes</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Technical Director, Smart Home Solutions Architecture at Synaptics</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-abdel"> <span style="font-weight: 600;">Biography:</span> Abdel Younes is currently leading the AI/ML system architecture team at Synaptics and the development of the SyNAP framework that supports AI on Synaptics edge SoC. From Aerospace Research to AI, going through OpenSource, Deeply Embedded SW development and Multimedia, Abdel Younes has 20+ years of experience leading projects and teams in various industries. He received his MS in Telecommunications and Electronics and his Ph.D. in Signal Processing and Telecommunication from the French Civil Aviation Academy (ENAC).</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> New AI-at-the-edge processors with improved efficiencies and flexibility are unleashing a huge opportunity to democratize computer vision broadly across all markets, enabling edge AI devices with small, low-cost, low-power cameras. Synaptics has embarked on a roadmap of edge-AI DNN processors targeted at a range of real-time computer vision and multimedia applications. In this talk, we will describe the hardware architecture used in Synaptics VideoSmart™ VS600 family with embedded built-in Neural Processor Unit and we will show how the SyNAP™ Software framework enables lightweight and optimized AI applications such as real-time Video SuperResolution upscaling.</p>
		</div>

	</div>
		
	<div class="accepted-paper" style="margin-top: 4vw; margin-bottom: 4vw;">
	<hr></br>
		<div class="block-1"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">12:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;">Break & Lunch</span> </span></p></br></br>
		</div>
	<hr>
	</div>
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";' src="assets/speakers/Cao_Xiaoqi.jpg">
			<a href="https://www.huawei.com/" target="_blank"><img style="margin-top: 0.7vw; width: 80%;" src="assets/organizers/huawei.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">Performance Test of AI Server Systems </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">12:30 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Cao
Xiaoqi</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior Engineer, Huawei</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-empty"> <span style="font-weight: 600;">Biography:</span></br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> This talk provides an overview to an  IEEE standard "P2937 Standard for performance benchmarking for AI server systems" under development. The standard provides a set of metrics,  workloads and rules for not only benchmarking but also finding the possible bottlenecks of systems' performance for optimization. Hard issues, such as energy cost isolation, in performance test paradigm are also included in this talk.</p>
		</div>

	</div>
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-peymen").style.display="inline-block";' src="assets/speakers/Milanfar.jpg">
			<a href="https://research.google/" target="_blank"><img style="margin-top: 0.7vw; width: 70%;" src="assets/organizers/google.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">Denoising as a Building Block for Imaging, Inverse Problems, and Machine Learning </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">13:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-peymen").style.display="inline-block";'>Peyman Milanfar</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Principal Scientist / Director at Google Research</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-peymen"> <span style="font-weight: 600;">Biography:</span> Peyman is a Principal Scientist / Director at Google Research, where he leads the Computational Imaging team. Prior to this, he was a Professor of Electrical Engineering at UC Santa Cruz from 1999-2014. He was Associate Dean for Research at the School of Engineering from 2010-12. From 2012-2014 he was on leave at Google-x, where he helped develop the imaging pipeline for Google Glass. Most recently, Peyman's team at Google developed the digital zoom pipeline for the Pixel phones, which includes the multi-frame super-resolution (Super Res Zoom) pipeline, and the RAISR upscaling algorithm.</br></br>

Peyman received his undergraduate education in electrical engineering and mathematics from the University of California, Berkeley, and the MS and PhD degrees in electrical engineering from the Massachusetts Institute of Technology. He holds 20 patents. He founded MotionDSP, which was acquired by Cubic Inc.</br></br>

Peyman has been keynote speaker at numerous technical conferences including Picture Coding Symposium (PCS), SIAM Imaging Sciences, SPIE, and the International Conference on Multimedia (ICME). Along with his students, he has won several best paper awards from the IEEE Signal Processing Society, including the 2021 Best paper award for "Kernel Regression in Image Processing and Reconstruction".</br></br>

He is a Distinguished Lecturer of the IEEE Signal Processing Society, and a Fellow of the IEEE "for contributions to inverse problems and super-resolution in imaging."</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Denoising is one of the oldest problems in imaging. In the last decade, the quality of denoising algorithms has reached phenomenal levels – almost as good as we can ever hope. There are thousands of papers on this topic, and their scope is vast and approaches so diverse that putting them in some order (as I will do) is both useful and challenging. I'll describe what we can say about this general class of operators, and what makes them so special. I will argue that denoising is increasingly important, not just as a process for removing noise, but especially now as a core engine and building block for much more complex tasks in imaging, inverse problems, and ML.</p>
		</div>

	</div>

		<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";' src="assets/speakers/yikang_li.jpg">
			<a href="https://www.oppo.com/" target="_blank"><img style="margin-top: 0.7vw; width: 70%;" src="assets/organizers/oppo.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">CLIP4Hashing: Unsupervised Deep Hashing for Cross-Modal Video-Text Retrieval </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">13:30 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Yikang Li</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Research Scientist at OPPO</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-empty"> <span style="font-weight: 600;">Biography:</span></br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> With explosively increasing amount of data on social media platforms like Twitter and TikTok, fast and accurate cross-modal retrieval on mobile devices is gaining much attention. Compared with the traditional cross-modal retrieval algorithms that work in the continuous feature space, hashing-based algorithms can retrieve data faster and more efficiently. However, most existing algorithms have difficulties in seeking or constructing a well-defined joint semantic space. In this paper, an unsupervised deep cross-modal video-text hashing approach, CLIP4Hashing, is proposed, which mitigates the difficulties in bridging between different modalities in the Hamming space through building a single hashing net by employing the pre-trained CLIP model. The approach is enhanced by two novel techniques, the dynamic weighting strategy and the design of the min-max hashing layer, which are found to be the main sources of the performance gain. With evaluation using three challenging video-text benchmark datasets, we demonstrate that CLIP4Hashing is able to significantly outperform existing state-of-the-art hashing algorithms. Additionally, with larger bit sizes (e.g., 2048 bits), CLIP4Hashing can even deliver competitive performance compared with the results based on non-hashing features.</p>
		</div>

	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">13:45 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span>RenderSR: A Lightweight Super-Resolution Model for Mobile Gaming Upscaling </span></p></br>
			<p class="authors"></p>Tingxing Dong, Hao Yan, Mayank Parasar, Raun Krisch</p></br>
			<p class="affiliation">☉ &nbsp;Samsung, South Korea</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">14:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span>An Efficient Hybrid Model for Low-light Image Enhancement in Mobile Devices </span></p></br>
			<p class="authors"></p>Zhicheng Fu, Miao Song, Chao Ma, Joseph V Nasti, Vivek Tyagi, Wei Tang </p></br>
			<p class="affiliation">☉ &nbsp;Lenovo Research, Motorola & University of Illinois, USA</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">14:15 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> PhoneDepth: A Dataset for Monocular Depth Estimation on Mobile Devices</span></p></br>
			<p class="authors"></p>Fausto Tapia Benavides, Andrey Ignatov, Radu Timofte</p></br>
			<p class="affiliation">☉ &nbsp;ETH Zurich, Switzerland</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">14:30 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> SMM-Conv: Scalar Matrix Multiplication with Zero Packing for Accelerated Convolution</span></p></br>
			<p class="authors"></p>Amir Ofir, Gil Ben-Artzi</p></br>
			<p class="affiliation">☉ &nbsp;Ariel University, Israel</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">14:45 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> Update Compression for Deep Neural Networks on the Edge</span></p></br>
			<p class="authors"></p>Bo Chen, Ali Bakhshi, Brian Ng, Gustavo Batista, Tat-Jun Chin</p></br>
			<p class="affiliation">☉ &nbsp;University of Adelaide & University of New South Wales, Australia</p></br>
		</div>
	</div>
	
	<div class="accepted-paper" >
		<div class="block-1"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">15:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;">Wrap Up & Closing</span> </span></p></br>
		</div>
	</div>
	
</div>


<div id="challenges">
	<p class="title"> CHALLENGES </p>
		
	<a href="https://codalab.lisn.upsaclay.fr/competitions/1756" target="_blank">
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> MediaTek Dimensity APU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/mediatek.png"></td></tr></table>
			</div>				
		</div>
	</a>

		
	<a href="https://codalab.lisn.upsaclay.fr/competitions/1755" target="_blank">
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Synaptics Dolphin NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/synaptics.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://codalab.lisn.upsaclay.fr/competitions/1759" target="_blank">
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/oppo.png"></td></tr></table>
			</div>				

		</div>
	</a>

	<a href="https://codalab.lisn.upsaclay.fr/competitions/1760" target="_blank">
		<div class="challenge">
			
			<p class="title"> Bokeh Effect Rendering <p>		
			<img src="assets/challenges/title_bokeh.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali GPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/huawei.png"></td></tr></table>
			</div>				
		</div>
	</a>

	<a href="https://platform.difficu.lt" target="_blank">
		<div class="challenge">
			
			<p class="title"> Depth Estimation <p>		
			<img src="assets/challenges/title_depth.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 4</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/raspberry.png"></td></tr></table>
			</div>				
		</div>
	</a>

</div>
	
<div id="challenges">
	<p class="title"> PREVIOUS CHALLENGES (2021) </p>

	<a href="https://arxiv.org/pdf/2105.07809.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28054 -->
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> MediaTek Dimensity APU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/mediatek.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.08629.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28120 -->
		<div class="challenge">
			
			<p class="title"> Image Denoising <p>		
			<img src="assets/challenges/title_denoising.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Exynos Mali GPU </td></tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/samsung.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.07825.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28119 -->
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Synaptics Dolphin NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/synaptics.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://arxiv.org/pdf/2105.08826.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28112 -->
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/oppo.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.08630.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28122 -->
		<div class="challenge">
			
			<p class="title"> Depth Estimation <p>		
			<img src="assets/challenges/title_depth.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 4</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/raspberry.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://arxiv.org/pdf/2105.08819.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28113 -->
		<div class="challenge">
			
			<p class="title"> Camera Scene Detection <p>		
			<img src="assets/challenges/title_scene.png">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Apple Bionic </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
</div>

<div id="runtime">
	<p class="title"> RUNTIME VALIDATION </p>
	<p class="description">
	In each MAI 2022 challenge track, the participants have a possibility to check the runtime of their solutions remotely on the target platforms. For this, the converted <span style="font-style: italic;">TensorFlow Lite</span> models should be uploaded to a <span style="font-weight: bold;">special web-server</span>, and their runtime on the actual target devices will be returned instantaneously or withing 24 hours, depending on the track. The detailed model conversion instructions and links can be found in the corresponding challenges.
	</br>
	</br>
	Besides that, we strongly encourage the participants to check the speed and RAM consumption of the obtained models locally on your own Android devices. This will allow you to perform model profiling and debugging faster and much more efficiently. To do this, one can use <a href="http://ai-benchmark.com/" target="_blank" class="link_red_dotted">AI Benchmark application</a> allowing you to load a custom TFLite model and run it with various acceleration options, including <span style="font-style: italic;">CPU</span>, <span style="font-style: italic;">GPU</span>, <span style="font-style: italic;">DSP</span> and <span style="font-style: italic;">NPU</span>:
	</br>
	</br>
	<span class="instruction_item">1.&nbsp; Download AI Benchmark from the <a href="https://play.google.com/store/apps/details?id=org.benchmark.demo" target="_blank" class="link_red_dotted">Google Play</a> / <a href="https://ai-benchmark.com/download.html" target="_blank" class="link_red_dotted">website</a> and run its standard tests.</span></br>
	<span class="instruction_item">2.&nbsp; After the end of the tests, enter the <span style="font-weight: bold;">PRO Mode</span> and select the <span style="font-weight: bold;">Custom Model</span> tab there.</span></br>
	<span class="instruction_item">3.&nbsp; Rename the exported TFLite model to <span style="font-style: italic;">model.tflite</span> and put it into the <span style="font-weight: bold;">Download</span> folder of your device.</span></br>
	<span class="instruction_item">4.&nbsp; Select your mode type, the desired acceleration / inference options and run the model.</span></br>
	</br>
	You can find the screenshots demonstrating these 4 steps below:</br>
	</p>
	<img src="assets/img/ai_benchmark_custom.png">
	
</div>

<div id="organizers">
	<p class="title"> ORGANIZERS </p>	
	
	<p class="tba">[ TO BE ANNOUNCED ]</p>

	<!--<img style="width: 100%;" src="assets/organizers/organizers.png"></td><td><span class="grey"-->

</div>

<div id="dates">
	<p class="title"> TIMELINE</p>
	
	<table class="timeline">
	    <thead>
	        <tr>
	            <th style="width: 64%;">Workshop Event</th>
	            <th>Date &nbsp; [ 5pm Pacific Time, 2022 ]</th>
	        </tr>
	    </thead>
	 	<tbody>
	        <tr><td>Website online</td><td>January 7</td></tr>
	        <tr><td>Paper submission server online </td><td> March 5 </td></tr>
	        <tr><td>Paper submission deadline </td><td><span style="color: rgb(234, 29, 111);">March 20</span></td></tr>
			<tr><td>Paper decision notification </td><td>April 14</td></tr>
	        <tr><td>Camera ready deadline </td><td>April 19</td></tr>
	       	<tr><td>Workshop day </td><td><span style="color: rgb(234, 29, 111);">June 20</span></td></tr>
	    </tbody>
	</table>
	
</div>

<div id="papers">
	<p class="title"> CALL FOR PAPERS </p>
	
	<p class="description">
	Being a part of <span style="font-weight: bold;">CVPR 2022,</span> we invite the authors to submit high-quality original papers proposing various machine learning based solutions for mobile, embedded and IoT platforms. The topics of interest cover all major aspects of AI and deep learning research for mobile devices including, but not limited to:

	</p>
	
	<table>
	
		<tr>
			<th style="width: 50%;"></th>
			<th style="width: 50%;"></th>
		</tr>
	
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Efficient deep learning models for mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Image / video super-resolution on low-power hardware</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; General smartphone photo and video enhancement</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Deep learning applications for mobile camera ISPs</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Fast image classification / object detection algorithms</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Real-time semantic image segmentation</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Image or sensor based identity recognition</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Activity recognition using smartphone sensors</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Depth estimation w/o multiple cameras</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Portrait segmentation / bokeh effect rendering</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Perceptual image manipulation on mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; NLP models optimized for mobile inference</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Artifacts removal from mobile photos / videos</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; RAW image and video processing</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Low-power machine learning inference</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Machine and deep learning frameworks for mobile devices</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; AI performance evaluation of mobile and IoT hardware</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Industry-driven applications related to the above problems</p></td>
		</tr>
		
	</table>
	
	<p class="description">
	To ensure high quality of the accepted papers, all submissions will be evaluated by research and industry experts from the corresponding fields.
	All accepted workshop papers will be published in the <span style="font-weight: bold;">CVPR 2022 Workshop Proceedings</span> by <a href="https://openaccess.thecvf.com/CVPR2020_workshops/CVPR2020_w31" target="_blank" style="font-style: italic; text-decoration-style: dotted; color: rgb(23, 142, 210);">Computer Vision Foundation Open Access</a> and <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" style="font-style: italic; text-decoration-style: dotted; color: rgb(23, 142, 210);">IEEE Xplore Digital Library.</a>
	The authors of the best selected papers will be invited to present their work during the actual workshop event at CVPR 2022.
	</br></br>
	The detailed submission instructions and guidelines can be found <a href="#submission" style="text-decoration: none; color: rgb(23, 142, 210);">here.</a>
	</p>
	
</div>

<div id="submission">
	<p class="title"> SUBMISSION DETAILS </p>
	
	<table class="timeline" style="margin-top: 4vw;">

	 	<tbody>

	        <tr><td style="width: 26%;">Format and paper length </td>
	        	<td class="submission_details">A paper submission has to be in English, in pdf format, and at most 8 pages (excluding references) in double column. The paper format must follow the same guidelines as for all CVPR 2022 submissions: <a href="https://cvpr2022.thecvf.com/author-guidelines" target="_blank" class="link_red_dotted">https://cvpr2022.thecvf.com/author-guidelines</a> </td></tr>

			<tr><td>Author kit </td>
	        	<td class="submission_details">The author kit provides a LaTeX2e template for paper submissions. Please refer to this kit for detailed formatting instructions: <a href="https://cvpr2022.thecvf.com/sites/default/files/2021-10/cvpr2022-author_kit-v1_1-1.zip" class="link_red_dotted">https://cvpr2022.thecvf.com/sites/default/files/2021-10/cvpr2022-author_kit-v1_1-1.zip</a>
	        	 </td></tr>

	        <tr><td>Double-blind review policy </td>
	        	<td class="submission_details">The review process is double blind. Authors do not know the names of the chair / reviewers of their papers. Reviewers do not know the names of the authors. </td></tr>
	        	
	        <tr><td>Dual submission policy </td>
	        	<td class="submission_details">Dual submission is allowed with CVPR2022 main conference only. If a paper is submitted also to CVPR and accepted, the paper cannot be published both at the CVPR and the workshop.
	        	 </td></tr>

			<tr><td>Proceedings </td>
	        	<td class="submission_details">Accepted and presented papers will be published after the conference in ECCV Workshops proceedings together with the ECCV 2022 main conference papers.
	        	 </td></tr>

			<tr><td>Submission site </td>
	        	<td class="submission_details">
	        	<a href="https://cmt3.research.microsoft.com/MAI2022" target="_blank" class="link_red_dotted">https://cmt3.research.microsoft.com/MAI2022</a>
	        	 </td></tr>


	    </tbody>
	</table>
	
</div>

<div id="live">
	<p class="title"> DEEP LEARNING ON MOBILE DEVICES: TUTORIAL </p>
	
	<iframe style="width: 50vw; height: 28.125vw;" src="https://www.youtube.com/embed/tdpmvy2Cab0?start=795" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>></iframe>
	
	<!--p style="margin-top: 2.4vw; font-size: 0.98vw;"> Join the main workshop Zoom conference: <a href="https://ethz.zoom.us/j/67422434389" target="_blank" style="color: rgb(234, 29, 111); text-decoration-style: dotted;">https://ethz.zoom.us/j/67422434389</a></p-->
	
	<p style="margin-top: 2.4vw; font-size: 0.98vw;"> Have some questions?&nbsp; Leave them on the <a href="https://ai-benchmark.net/" target="_blank" style="color: rgba(23, 142, 210, 1.0); text-decoration-style: dotted;">AI Benchmark Forum</a></p>
</div>

<div id="schedule">
	<p class="title"> SCHEDULE </p>	
	<p class="tba">[ TO BE ANNOUNCED ]</p>	
</div>

<div id="contacts">
	<p class="title">CONTACTS</p>
	
	<table>
		<tr>
			
			<td><img style="width: 10vw; vertical-align: top; margin-left: -0.6vw; border-radius: 0.32vw;" src="assets/img/andrey.jpg"></td>
			
			<td style="text-align: left; vertical-align: top; text-align: left; padding-left: 0.9vw;">
				<p style="margin-top: 1.2vw;"><a target="_blank" href="https://www.linkedin.com/in/andrey-ignatov" style="color: #ea1d6f; font-size: 1.2vw; text-decoration: none;">
					Andrey Ignatov
				</a></p>
				<p style="margin-top: 0.7vw;">Computer Vision Lab </p>
				<p>ETH Zurich, Switzerland </p>
				<p style="margin-top: 0.5vw; font-size: 0.9vw; color: #ea1d6f;">andrey@vision.ee.ethz.ch</p>
			</td>
			
			<td><img style="width: 10vw; vertical-align: top; margin-left: 2.6vw; border-radius: 0.32vw;" src="assets/img/radu.jpg"></td>
			
			<td style="text-align: left; vertical-align: top; text-align:left; padding-left: 0.5vw;">
				<p style="margin-top: 1.2vw;"><a target="_blank" href="http://www.vision.ee.ethz.ch/~timofter/" style="color: #ea1d6f; font-size: 1.2vw; text-decoration: none;">
					Radu Timofte
				</a></p>
				<p style="margin-top: 0.7vw;">Computer Vision Lab </p>
				<p>ETH Zurich, Switzerland </p>
				<p style="margin-top: 0.5vw; font-size: 0.9vw; color: #ea1d6f;">timofter@vision.ee.ethz.ch</p>
			</td>
						
		</tr>
	</table>
	
</div>

<div id="about">
	<div id="content">
		<p> Computer Vision Laboratory, ETH Zurich</p>
		<p style="color: #ea1d6f;">Switzerland, 2022</p>
	</div>
</div>

<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script>
<script src="js/main.js"></script>

</body>

</html>
