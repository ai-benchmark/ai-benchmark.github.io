<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en">

<head>

    <meta charset="utf-8">
    <title> MAI 2023 Workshop </title>
    <link rel="stylesheet" href="style_challenge.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,700italic,400italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i" rel="stylesheet"> 
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <script src="js/modernizr.js"></script>
    <meta name="viewport" content="width=device-width">
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-118781498-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-118781498-1');
	  
	</script>
    
</head>

<body data-spy="scroll" data-offset="0" data-target="#navbar-main">

<nav>
	<ul class="left_bar">
		<a href="index.html"> <img src="assets/img/splash.png"> </a>
	</ul>
	<ul class="middle_bar">
		<a href="#live"> <li style="color: rgb(238, 0, 106);"> LIVE <svg style="height: 0.6vw; width: 0.6vw; margin-left: 0.4vw;" class="blinking"><circle cx="0.3vw" cy="0.3vw" r="0.3vw" fill="rgb(238, 0, 106)" /></svg>
</li></a>
		<a href="#schedule"> <li> SCHEDULE </li></a>
		<a href="#challenges"> <li> CHALLENGES </li></a>
		<!--a href="#papers"> <li> PAPERS </li></a-->
		<!--a href="#organizers"> <li> ORGANIZERS </li></a-->
		<a href="#dates"> <li> DATES </li></a>
		<a href="#tutorial"> <li> TUTORIAL </li></a>
		<a href="#runtime"> <li> RUNTIME VALIDATION</li></a>
		<a href="#contacts"> <li> CONTACTS </li></a>
	</ul>
	<ul class="right_bar">
		<a href="#submission" class="submit_button"> <li> SUBMIT PAPER </li></a>
	</ul>
</nav>

<img src="assets/img/mai_logo_2023.jpg" style="width: 100%; margin-top: 4.0vw;">

<div id="workshop">

		<div class="description"> 	
		
		<p> Over the past years, mobile AI-based applications are becoming more and more ubiquitous. Various deep learning models can now be found on any mobile device, starting from smartphones running portrait segmentation, image enhancement, face recognition and natural language processing models, to smart-TV boards coming with sophisticated image super-resolution algorithms. The performance of mobile NPUs and DSPs is also increasing dramatically, making it possible to run complex deep learning models and to achieve fast runtime in the majority of tasks.
		</p>

		<p style="margin-top: 1.2vw;"> While many research works targeted at efficient deep learning models have been proposed recently, the evaluation of the obtained solutions is usually happening on desktop CPUs and GPUs, making it nearly impossible to estimate the actual inference time and memory consumption on real mobile hardware. To address this problem, we introduce the first Mobile AI Workshop, where all deep learning solutions are developed for and evaluated on mobile devices.
		</p>
		
		<p style="margin-top: 1.2vw;"> Due to the performance of the last-generation mobile AI hardware, the topics considered in this workshop will go beyond the simple classification tasks, and will include such challenging problems as <span style="font-style: italic;">image denoising, HDR photography, accurate depth estimation, learned image ISP pipeline, real-time image and video super-resolution</span>. All information about the challenges, papers, invited talks and workshop industry partners is provided below.
		</p>
	</div>
<div>

<div id="live">
	<p class="title"> LIVE SESSION IN ZOOM </p>
	
	<p style="margin-top: 2.4vw; font-size: 0.98vw;"> Join the workshop using the following <a href="https://us06web.zoom.us/j/84208157640?pwd=RVJ3ZVdWUisyaTlTeTVnb1c2OHJWZz09" target="_blank" style="color: rgba(23, 142, 210, 1.0); text-decoration-style: dotted;">link</a> ( Password: feltcars )</p>
		
</div>


<div id="schedule">
	<p class="title"> SCHEDULE </p>	
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-andrey").style.display="inline-block";' src="assets/speakers/andrey.jpg">
			<a href="https://vision.ee.ethz.ch/" target="_blank"><img style="margin-top: 0.7vw; width: 75%" src="assets/organizers/eth-zurich.png"></a>
			<!--a href="https://ai-benchmark.com/" target="_blank"><img style="margin-top: 0.7vw; width: 100%" src="assets/organizers/ai_witchlabs.png"></a-->
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;  line-height: 1.4vw;">Deep Learning on Mobile Devices:&nbsp; <span style="font-weight: 500;">What's New in 2023?</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">08:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-andrey").style.display="inline-block";'>Andrey Ignatov</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">AI Benchmark Project Lead, ETH Zurich</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-andrey"> <span style="font-weight: 600;">Biography:</span> Andrey Ignatov is the project lead of the AI Benchmark initiative targeted at performance evaluation of mobile, IoT and desktop hardware at ETH Zurich, Switzerland. His PhD research was aimed at designing efficient image processing models for smartphone NPUs / DSPs and developing the next-generation deep learning based mobile camera ISP solution. He is the lecturer on Deep Learning for Smartphones course at ETH Zurich and the main author of the AI Benchmark papers describing the current state of deep learning and AI hardware acceleration on mobile devices. He is a co-founder of the AI Witchlabs and co-organizer of the NTIRE and AIM events. His main line of research is focused on image restoration and automatic image quality enhancement, adaptation of AI applications for mobile devices and benchmarking machine learning hardware.</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> In this tutorial, we will review the recent Android AI software stack updates, and will talk about the performance of the latest mobile chipsets from Qualcomm, MediaTek, Google, Samsung and Unisoc released during the past year. We will also discuss the power efficiency of mobile chipsets and their NPUs, and will analyze their energy consumption for a number of typical AI workloads.</p>
		</div>

	</div>
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer; margin-bottom: 0.4vw;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Hsien-Kai_Kuo.jpg">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Yu-Syuan_Xu.jpg">
			<a href="https://www.mediatek.com/" target="_blank"><img style="margin-top: 0.7vw; width: 80%" src="assets/organizers/mediatek.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;">Mobile AI Evolution - the Trend and Challenge</p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">08:40 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";'>Hsien-Kai Kuo</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior Department Manager, MediaTek Inc.</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">09:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";'>Yu-Syuan Xu</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior AI Engineer, MediaTek Inc.</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-allen"> <span style="font-weight: 600;">Biography:</span> Hsien-Kai Kuo is a Senior Department Manager at MediaTek Inc. He received the PhD degree in electrical engineering from National Chiao Tung University, Taiwan, in 2014. He serves as a reviewer/author of several papers in CVPR’23/20, TC’16, CAL’15-16, TODAES’14, DAC’14, ASP-DAC’12-14. His research interests include deep learning, computer vision, computer architecture, electronic design automation, parallel and heterogeneous computing, multicore and GPGPU. He is also a member of IEEE. You can learn more about his work on his Google Scholar profile at <a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank">https://scholar.google.com/citations?user=EeVsWaUAAAAJ</a>.

</br></br>

Yu-Syuan Xu is an Senior AI engineer at MediaTek. His research mainly focuses on deep learning and its applications, specifically on developing and deploying the latest and trendiest AI models on edge devices equipped with MediaTek APU. He is responsible for co-designing AI accelerators from an algorithmic point of view. He received both his B.S. degree and M.S. degree in computer science from National Tsing Hua University in 2017 and 2018, respectively, under the guidance of Prof. Chun-Yi Lee. You can learn more about his work on his website at <a href="https://xusean0118.github.io/" target="_blank">https://xusean0118.github.io/</a> and on his Google Scholar profile at <a href="https://scholar.google.com/citations?user=QA3VSaYAAAAJ" target="_blank">https://scholar.google.com/citations?user=QA3VSaYAAAAJ</a>.

</br></br>
</p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Mobile AI is evolving from short-burst applications like photos to sustained ones such as videos, games, and VR/MR. Recently, Transformer, designed for natural language processing, has become the foundation network of generative AI, and has shown superior performance in vision and voice applications for mobile devices. In this talk,  we will introduce mobile AI applications, with a particular focus on the advantages and challenges of utilizing transformers. However, running vision transformer models with higher-resolution data under a limited power budget is the most critical challenge for mobile AI applications. We will set up a vision transformer based video super resolution challenge to promote the efficient deployment of transformer.
 </p>
		</div>
	</div>
	

	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";' src="assets/speakers/LanFu.jpg">
			<a href="https://www.oppo.com/" target="_blank"><img style="margin-top: 0.7vw; width: 70%;" src="assets/organizers/oppo.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">Make the World a Clearer Place </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">09:30 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Lan Fu</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Senior Research Engineer, OPPO US R&D</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-empty"> <span style="font-weight: 600;">Biography:</span></br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Although deep learning based video super-resolution methods have achieved promising performance, they are so computation intensive that it is impractical to deploy them on edge devices demanding higher efficiency. Moreover, various degradations in real-world applications pose a great challenge to their generalization abilities. In this talk, we present a video super-resolution solution on mobile devices, which can be real-time and deal with a wide range of degradations as well. Extensive experiments demonstrate its state-of-the-art performance on a public video super-resolution dataset. We have successfully delivered it to OPPO smartphones.</p>
		</div>

	</div>

	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";' src="assets/speakers/JiezhangCao.jpg">
			<a href="https://vision.ee.ethz.ch/" target="_blank"><img style="margin-top: 0.7vw; width: 75%" src="assets/organizers/eth-zurich.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw; line-height: 1.4vw;">CiaoSR:&nbsp; Arbitrary-Scale Image Super-Resolution </p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">09:55 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-empty-").style.display="inline-block";'>Jiezhang Cao</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">Computer Vision Researcher, ETH Zurich</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-empty"> <span style="font-weight: 600;">Biography:</span></br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> Learning continuous image representations is recently gaining popularity for image super-resolution because of its ability to reconstruct high-resolution images with arbitrary scales from low-resolution inputs. Existing methods mostly ensemble nearby features to predict the new pixel at any queried coordinate in the SR image. Such a local ensemble suffers from some limitations: i) it has no learnable parameters and it neglects the similarity of the visual features; ii) it has a limited receptive field and cannot ensemble relevant features in a large field which are important in an image. To address these issues, we propose a continuous implicit attention-in-attention network, called CiaoSR. We explicitly design an implicit attention network to learn the ensemble weights for the nearby local features. Furthermore, we embed a scale-aware attention in this implicit attention network to exploit additional non-local information. CiaoSR achieves the state-of-the-art performance on the arbitrary-scale SR task, the effectiveness of the method is also demonstrated on the real-world SR setting.</p>
		</div>

	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">10:20 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span>QuickSRNet: Plain Single-Image Super-Res Architecture for Faster Inference on Mobile Platforms </span></p></br>
			<p class="authors"></p>Guillaume J. F. Berger, Manik Dhingra, Antoine Mercier, Yashesh Savani, Sunny P Panchal, Fatih Porikli</p></br>
			<p class="affiliation">☉ &nbsp;Qualcomm Technologies Inc., USA</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">10:40 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span>Real-time Segmenting Human Portrait at Anywhere </span></p></br>
			<p class="authors"></p>Ruifeng Yuan, Yuhao Cheng, Yiqiang Yan, Haiyan Liu </p></br>
			<p class="affiliation">☉ &nbsp;Lenovo Research, China</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">11:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> VideoMatt: A Simple Baseline for Accessible Real-Time Video Matting</span></p></br>
			<p class="authors"></p>Jiachen Li, Marianna Ohanyan, Vidit Goel, Shant Navasardyan, Yunchao Wei, Humphrey Shi</p></br>
			<p class="affiliation">☉ &nbsp;University of Oregon & UIUC & BJTU & Picsart AI Research, USA & China</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">11:20 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> DIFT: Dynamic Iterative Field Transforms for Memory Efficient Optical Flow</span></p></br>
			<p class="authors"></p>Risheek Garrepalli, Jisoo Jeong, Rajeswaran Ravindran, Fatih Porikli, Jamie Menjay Lin</p></br>
			<p class="affiliation">☉ &nbsp;Qualcomm AI Research, USA</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">11:40 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications</span></p></br>
			<p class="authors"></p>Mustafa Munir, William Avery, Radu Marculescu</p></br>
			<p class="affiliation">☉ &nbsp;The University of Texas at Austin, USA</p></br>
		</div>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">12:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"></span> High-efficiency Device-Cloud Collaborative Transformer Model</span></p></br>
			<p class="authors"></p>Penghao Jiang, Ke Xin, Chunxi Li, Yinsi Zhou</p></br>
			<p class="affiliation">☉ &nbsp;The Australian National University & University of Technology Sydney, Australia</p></br>
		</div>
	</div>
	
		<div class="accepted-paper" style="margin-top: 4vw; margin-bottom: 4vw;">
	<hr></br>
		<div class="block-1"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">12:20 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;">Break & Lunch</span> </span></p></br></br>
		</div>
	<hr>
	</div>
	
	<div class="accepted-paper" >
		<div class="block-1"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">14:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;">Poster Session and In-Person Discussion</span> </span></p></br>
		</div>
	</div>
	
</div>
	
<div id="challenges">
	<p class="title"> PREVIOUS CHALLENGES &nbsp;(2022) </p>
		
	<a href="https://polybox.ethz.ch/index.php/s/t6raVPNRmpKxfVU" target="_blank">
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1756" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> MediaTek Dimensity APU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/mediatek.png"></td></tr></table>
			</div>				
		</div>
	</a>

	<a href="https://polybox.ethz.ch/index.php/s/PilVu1xvj9Kmmfz" target="_blank">		
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1755" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Synaptics Dolphin NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/synaptics.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://polybox.ethz.ch/index.php/s/CxqzyzdBn0AgSZc" target="_blank">		
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1759" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/oppo.png"></td></tr></table>
			</div>				

		</div>
	</a>

	<a href="https://polybox.ethz.ch/index.php/s/jt8sTpOcP8SmgBd" target="_blank">
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1760" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Bokeh Effect Rendering <p>		
			<img src="assets/challenges/title_bokeh.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali GPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/huawei.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://polybox.ethz.ch/index.php/s/1UM1O9RviLi5vMM" target="_blank">
	<!--a href="https://platform.difficu.lt" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Depth Estimation <p>		
			<img src="assets/challenges/title_depth.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 4</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/raspberry.png"></td></tr></table>
			</div>				
		</div>
	</a>

</div>
	
<div id="challenges">
	<p class="title"> PREVIOUS CHALLENGES &nbsp;(2021) </p>

	<a href="https://arxiv.org/pdf/2105.07809.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28054 -->
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> MediaTek Dimensity APU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/mediatek.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.08629.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28120 -->
		<div class="challenge">
			
			<p class="title"> Image Denoising <p>		
			<img src="assets/challenges/title_denoising.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Exynos Mali GPU </td></tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/samsung.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.07825.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28119 -->
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Synaptics Dolphin NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/synaptics.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://arxiv.org/pdf/2105.08826.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28112 -->
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/oppo.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.08630.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28122 -->
		<div class="challenge">
			
			<p class="title"> Depth Estimation <p>		
			<img src="assets/challenges/title_depth.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 4</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/raspberry.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://arxiv.org/pdf/2105.08819.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28113 -->
		<div class="challenge">
			
			<p class="title"> Camera Scene Detection <p>		
			<img src="assets/challenges/title_scene.png">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Apple Bionic </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
</div>

<!--div id="organizers">
	<p class="title"> ORGANIZERS </p>	
	
	<p class="tba">[ TO BE ANNOUNCED ]</p>

	<!--<img style="width: 100%;" src="assets/organizers/organizers.png"></td><td><span class="grey">

</div-->

<div id="papers">
	<p class="title"> CALL FOR PAPERS </p>
	
	<p class="description">
	Being a part of <span style="font-weight: bold;">CVPR 2022,</span> we invite the authors to submit high-quality original papers proposing various machine learning based solutions for mobile, embedded and IoT platforms. The topics of interest cover all major aspects of AI and deep learning research for mobile devices including, but not limited to:

	</p>
	
	<table>
	
		<tr>
			<th style="width: 50%;"></th>
			<th style="width: 50%;"></th>
		</tr>
	
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Efficient deep learning models for mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Image / video super-resolution on low-power hardware</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; General smartphone photo and video enhancement</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Deep learning applications for mobile camera ISPs</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Fast image classification / object detection algorithms</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Real-time semantic image segmentation</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Image or sensor based identity recognition</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Activity recognition using smartphone sensors</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Depth estimation w/o multiple cameras</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Portrait segmentation / bokeh effect rendering</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Perceptual image manipulation on mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; NLP models optimized for mobile inference</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Artifacts removal from mobile photos / videos</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; RAW image and video processing</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Low-power machine learning inference</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Machine and deep learning frameworks for mobile devices</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; AI performance evaluation of mobile and IoT hardware</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Industry-driven applications related to the above problems</p></td>
		</tr>
		
	</table>
	
	<p class="description">
	To ensure high quality of the accepted papers, all submissions will be evaluated by research and industry experts from the corresponding fields.
	All accepted workshop papers will be published in the <span style="font-weight: bold;">CVPR 2022 Workshop Proceedings</span> by <a href="https://openaccess.thecvf.com/CVPR2020_workshops/CVPR2020_w31" target="_blank" style="font-style: italic; text-decoration-style: dotted; color: rgb(23, 142, 210);">Computer Vision Foundation Open Access</a> and <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" style="font-style: italic; text-decoration-style: dotted; color: rgb(23, 142, 210);">IEEE Xplore Digital Library.</a>
	The authors of the best selected papers will be invited to present their work during the actual workshop event at CVPR 2022.
	</br></br>
	The detailed submission instructions and guidelines can be found <a href="#submission" style="text-decoration: none; color: rgb(23, 142, 210);">here.</a>
	</p>
	
</div>

<div id="submission">
	<p class="title"> SUBMISSION DETAILS </p>
	
	<table class="timeline" style="margin-top: 4vw;">

	 	<tbody>

	        <tr><td style="width: 26%;">Format and paper length </td>
	        	<td class="submission_details">A paper submission has to be in English, in pdf format, and at most 8 pages (excluding references) in double column. The paper format must follow the same guidelines as for all CVPR 2022 submissions: <a href="https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines" target="_blank" class="link_red_dotted">https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines</a> </td></tr>

			<tr><td>Author kit </td>
	        	<td class="submission_details">The author kit provides a LaTeX2e template for paper submissions. Please refer to this kit for detailed formatting instructions: <a href="https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip" class="link_red_dotted">https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip</a>
	        	 </td></tr>

	        <tr><td>Double-blind review policy </td>
	        	<td class="submission_details">The review process is double blind. Authors do not know the names of the chair / reviewers of their papers. Reviewers do not know the names of the authors. </td></tr>
	        	
	        <tr><td>Dual submission policy </td>
	        	<td class="submission_details">Dual submission is allowed with CVPR2022 main conference only. If a paper is submitted also to CVPR and accepted, the paper cannot be published both at the CVPR and the workshop.
	        	 </td></tr>

			<tr><td>Proceedings </td>
	        	<td class="submission_details">Accepted and presented papers will be published after the conference in ECCV Workshops proceedings together with the ECCV 2022 main conference papers.
	        	 </td></tr>

			<tr><td>Submission site </td>
	        	<td class="submission_details">
	        	<a href="https://cmt3.research.microsoft.com/MAI2023" target="_blank" class="link_red_dotted">https://cmt3.research.microsoft.com/MAI2023</a>
	        	 </td></tr>


	    </tbody>
	</table>
	
</div>

<div id="dates">
	<p class="title"> TIMELINE</p>
	
	<table class="timeline">
	    <thead>
	        <tr>
	            <th style="width: 64%;">Workshop Event</th>
	            <th>Date &nbsp; [ 5pm Pacific Time, 2023 ]</th>
	        </tr>
	    </thead>
	 	<tbody>
	        <tr><td>Website online</td><td>January 17</td></tr>
	        <tr><td>Paper submission server online </td><td> February 20 </td></tr>
	        <tr><td>Paper submission deadline </td><td><span style="color: rgb(234, 29, 111);">March 10</span></td></tr>
			<tr><td>Paper decision notification </td><td>April</td></tr>
	        <tr><td>Camera ready deadline </td><td>April</td></tr>
	       	<tr><td>Workshop day </td><td><span style="color: rgb(234, 29, 111);">June 19</span></td></tr>
	    </tbody>
	</table>
	
</div>

<div id="tutorial">
	<p class="title"> DEEP LEARNING ON MOBILE DEVICES: TUTORIAL </p>
	
	<iframe style="width: 50vw; height: 28.125vw;" src="https://www.youtube.com/embed/tdpmvy2Cab0?start=795" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>></iframe>
		
	<iframe style="width: 50vw; height: 28.125vw; margin-top: 3.4vw;" src="https://www.youtube.com/embed/aaM_iZrW9Y4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	
	<p style="margin-top: 2.4vw; font-size: 0.98vw;"> Have some questions?&nbsp; Leave them on the <a href="https://ai-benchmark.net/" target="_blank" style="color: rgba(23, 142, 210, 1.0); text-decoration-style: dotted;">AI Benchmark Forum</a></p>
		
</div>

<div id="runtime">
	<p class="title"> RUNTIME VALIDATION </p>
	<p class="description">
	In each MAI 2022 challenge track, the participants have a possibility to check the runtime of their solutions remotely on the target platforms. For this, the converted <span style="font-style: italic;">TensorFlow Lite</span> models should be uploaded to a <span style="font-weight: bold;">special web-server</span>, and their runtime on the actual target devices will be returned instantaneously or withing 24 hours, depending on the track. The detailed model conversion instructions and links can be found in the corresponding challenges.
	</br>
	</br>
	Besides that, we strongly encourage the participants to check the speed and RAM consumption of the obtained models locally on your own Android devices. This will allow you to perform model profiling and debugging faster and much more efficiently. To do this, one can use <a href="http://ai-benchmark.com/" target="_blank" class="link_red_dotted">AI Benchmark application</a> allowing you to load a custom TFLite model and run it with various acceleration options, including <span style="font-style: italic;">CPU</span>, <span style="font-style: italic;">GPU</span>, <span style="font-style: italic;">DSP</span> and <span style="font-style: italic;">NPU</span>:
	</br>
	</br>
	<span class="instruction_item">1.&nbsp; Download AI Benchmark from the <a href="https://play.google.com/store/apps/details?id=org.benchmark.demo" target="_blank" class="link_red_dotted">Google Play</a> / <a href="https://ai-benchmark.com/download.html" target="_blank" class="link_red_dotted">website</a> and run its standard tests.</span></br>
	<span class="instruction_item">2.&nbsp; After the end of the tests, enter the <span style="font-weight: bold;">PRO Mode</span> and select the <span style="font-weight: bold;">Custom Model</span> tab there.</span></br>
	<span class="instruction_item">3.&nbsp; Rename the exported TFLite model to <span style="font-style: italic;">model.tflite</span> and put it into the <span style="font-weight: bold;">Download</span> folder of your device.</span></br>
	<span class="instruction_item">4.&nbsp; Select your mode type, the desired acceleration / inference options and run the model.</span></br>
	</br>
	You can find the screenshots demonstrating these 4 steps below:</br>
	</p>
	<img src="assets/img/ai_benchmark_custom.png">
	
</div>

<div id="contacts">
	<p class="title">CONTACTS</p>
	
	<table>
		<tr>
			
			<td><img style="width: 10vw; vertical-align: top; margin-left: -0.6vw; border-radius: 0.32vw;" src="assets/img/andrey.jpg"></td>
			
			<td style="text-align: left; vertical-align: top; text-align: left; padding-left: 0.9vw;">
				<p style="margin-top: 1.2vw;"><a target="_blank" href="https://www.linkedin.com/in/andrey-ignatov" style="color: #ea1d6f; font-size: 1.2vw; text-decoration: none;">
					Andrey Ignatov
				</a></p>
				<p style="margin-top: 0.7vw;">Computer Vision Lab </p>
				<p>ETH Zurich, Switzerland </p>
				<p style="margin-top: 0.5vw; font-size: 0.9vw; color: #ea1d6f;">andrey@vision.ee.ethz.ch</p>
			</td>
			
			<td><img style="width: 10vw; vertical-align: top; margin-left: 2.6vw; border-radius: 0.32vw;" src="assets/img/radu.jpg"></td>
			
			<td style="text-align: left; vertical-align: top; text-align:left; padding-left: 0.5vw;">
				<p style="margin-top: 1.2vw;"><a target="_blank" href="https://www.informatik.uni-wuerzburg.de/computervision/home/" style="color: #ea1d6f; font-size: 1.2vw; text-decoration: none;">
					Radu Timofte
				</a></p>
				<p style="margin-top: 0.7vw;">Computer Vision Laboratory </p>
				<p>University of Würzburg, Germany </p>
				<p style="margin-top: 0.5vw; font-size: 0.9vw; color: #ea1d6f;">radu.timofte@uni-wuerzburg.de</p>
			</td>
						
		</tr>
	</table>
	
</div>

<div id="about">
	<div id="content">
		<p> Computer Vision Laboratory, ETH Zurich</p>
		<p style="color: #ea1d6f;">Switzerland, 2023</p>
	</div>
</div>

<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script>
<script src="js/main.js"></script>

</body>

</html>
