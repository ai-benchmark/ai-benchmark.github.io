<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en">

<head>

    <meta charset="utf-8">
    <title> MAI 2026 Workshop </title>
    <link rel="stylesheet" href="style_challenge.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,700italic,400italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i" rel="stylesheet"> 
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <script src="js/modernizr.js"></script>
    <meta name="viewport" content="width=device-width">
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-118781498-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-118781498-1');
	  
	</script>
    
</head>

<body data-spy="scroll" data-offset="0" data-target="#navbar-main">

<nav>
	<ul class="left_bar">
		<a href="index.html"> <img src="assets/img/splash.png"> </a>
	</ul>
	<ul class="middle_bar">
		<!--a href="#live"> <li style="color: rgb(238, 0, 106);"> LIVE <svg style="height: 0.6vw; width: 0.6vw; margin-left: 0.4vw;" class="blinking"><circle cx="0.3vw" cy="0.3vw" r="0.3vw" fill="rgb(238, 0, 106)" /></svg>
</li></a-->
		<a href="#schedule"> <li> SCHEDULE </li></a>
		<a href="#challenges"> <li> CHALLENGES </li></a>
		<a href="#papers"> <li> PAPERS </li></a>
		<!--a href="#organizers"> <li> ORGANIZERS </li></a-->
		<a href="#dates"> <li> DATES </li></a>
		<a href="#tutorial"> <li> TUTORIAL </li></a>
		<a href="#runtime"> <li> RUNTIME VALIDATION</li></a>
		<a href="#contacts"> <li> CONTACTS </li></a>
	</ul>
	<ul class="right_bar">
		<a href="#submission" class="submit_button"> <li> SUBMIT PAPER </li></a>
	</ul>
</nav>

<img src="assets/img/mai_logo_2026.jpg" style="width: 100%; margin-top: 4.0vw;">

<div id="workshop">

		<div class="description"> 	
		
		<p> Over the past years, mobile AI-based applications are becoming more and more ubiquitous. Various deep learning models can now be found on any mobile device starting from smartphones running LLMs, image enhancement, portrait segmentation, face recognition and neural generation models, to IoT platforms performing real-time image classification or smart-TV boards coming with sophisticated image super-resolution algorithms. The performance of mobile NPUs and DSPs is also increasing dramatically, making it possible to run complex deep learning models and to achieve fast runtime in the majority of tasks.
		</p>

		<p style="margin-top: 1.2vw;"> While many research works targeted at efficient deep learning models have been proposed recently, the evaluation of the obtained solutions is usually happening on desktop CPUs and GPUs, making it nearly impossible to estimate the actual inference time and memory consumption on real mobile hardware. To address this problem, we introduce the first Mobile AI Workshop, where all deep learning solutions are developed for and evaluated on mobile devices.
		</p>
		
		<p style="margin-top: 1.2vw;"> Due to the performance of the last-generation mobile AI hardware, the topics considered in this workshop will go beyond the simple classification tasks, and will include such challenging problems as <span style="font-style: italic;">image denoising, efficient LLM and Stable Diffusion, learned image ISP pipeline, smartphone photo enhancement, real-time image and video super-resolution</span>. All information about the challenges, papers, invited talks and workshop industry partners is provided below.
		</p>
	</div>
<div>

<div id="challenges">
	<p class="title"> CHALLENGES:&nbsp; ONGOING </p>

	<a href="https://codalab.lisn.upsaclay.fr/competitions/21487" target="_blank">
		<div class="challenge">
			
			<p class="title"> 4K Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon 8 Elite G5 NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://www.codabench.org/competitions/8453/" target="_blank">
		<div class="challenge">
			
			<p class="title"> 4K Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Dimensity 9500 NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://codalab.lisn.upsaclay.fr/competitions/21558" target="_blank">
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
			
	<a href="https://codalab.lisn.upsaclay.fr/competitions/21868" target="_blank">
		<div class="challenge">
			
			<p class="title"> Efficient LLMs <p>		
			<img src="assets/challenges/title_llm.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 8GB</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://codalab.lisn.upsaclay.fr/competitions/21869" target="_blank">
		<div class="challenge">
			
			<p class="title"> Efficient Stable Diffusion <p>
			<img src="assets/challenges/title_sd.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Apple M4 Neural Engine </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://www.codabench.org/competitions/12934/" target="_blank">
		<!-- Codalab link: https://codalab.lisn.upsaclay.fr/competitions/21559 -->
		<div class="challenge">
			
			<p class="title"> Image Denoising <p>		
			<img src="assets/challenges/title_denoising.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td></tr></table>
				</div>			
				<hr>
				<table><tr><!--tdtd>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://codalab.lisn.upsaclay.fr/competitions/21562" target="_blank">
		<div class="challenge">
			
			<p class="title"> Bokeh Effect Rendering <p>		
			<img src="assets/challenges/title_bokeh.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://codalab.lisn.upsaclay.fr/competitions/21564" target="_blank">
		<div class="challenge">
			
			<p class="title"> RGB Photo Enhancement <p>		
			<img src="assets/challenges/title_rgb.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://www.codabench.org/competitions/12938/" target="_blank">
		<!-- Codalab link: https://codalab.lisn.upsaclay.fr/competitions/21561-->
		<div class="challenge">
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
</div>

<div id="challenges">
	<p class="title"> MAI 2025 CHALLENGE REPORTS </p>
	
	<a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Ignatov_Quantized_Image_Super-Resolution_on_Mobile_NPUs_Mobile_AI_2025_Challenge_CVPRW_2025_paper.pdf" target="_blank">
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Google Tensor TPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Ignatov_RGB_Photo_Enhancement_on_Mobile_GPUs_Mobile_AI_2025_Challenge_CVPRW_2025_paper.pdf" target="_blank">
		<div class="challenge">
			
			<p class="title"> RGB Photo Enhancement <p>		
			<img src="assets/challenges/title_rgb.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Ignatov_Learned_Smartphone_ISP_on_Mobile_GPUs_Mobile_AI_2025_Challenge_CVPRW_2025_paper.pdf" target="_blank">
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali / Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><!--td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td--></tr></table>
			</div>				
		</div>
	</a>
</div>


<!--div id="live">
	<p class="title"> LIVE </p>	
	
	<iframe style="width: 50vw; height: 28.125vw;" src="https://www.youtube.com/embed/hUC7SGEYsl0?si=T9Ch137afyJ_DzJn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	
	<p style="margin-top: 2.4vw; font-size: 0.98vw;"> Join the main workshop Zoom conference for a Q&A session: <a href="https://ethz.zoom.us/j/61668005465" target="_blank" style="color: rgb(234, 29, 111); text-decoration-style: dotted;">https://ethz.zoom.us/j/61668005465</a></p>
		
</div-->

<!--div id="schedule">
	<p class="title"> SCHEDULE </p>	
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-andrey").style.display="inline-block";' src="assets/speakers/andrey.jpg">
			<a href="https://vision.ee.ethz.ch/" target="_blank"><img style="margin-top: 0.7vw; width: 75%" src="assets/organizers/eth-zurich.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;  line-height: 1.4vw;">Deploying Deep Learning Models on Mobile NPUs and Beyond:&nbsp; <span style="font-weight: 500;">What's New in 2025?</span></p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">07:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-andrey").style.display="inline-block";'>Andrey Ignatov</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">AI Benchmark Project Lead, ETH Zurich</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-andrey"> <span style="font-weight: 600;">Biography:</span> Andrey Ignatov is the project lead of the AI Benchmark initiative targeted at performance evaluation of mobile, IoT and desktop hardware at ETH Zurich, Switzerland. His PhD research was aimed at designing efficient image processing models for smartphone NPUs / DSPs and developing the next-generation deep learning based mobile camera ISP solution. He is the lecturer on Deep Learning for Smartphones course at ETH Zurich and the main author of the AI Benchmark papers describing the current state of deep learning and AI hardware acceleration on mobile devices. He is a co-founder of the AI Witchlabs and co-organizer of the NTIRE and AIM events. His main line of research is focused on image restoration and automatic image quality enhancement, adaptation of AI applications for mobile devices and benchmarking machine learning hardware.</br></br></p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> In this tutorial, we will first recall all basic concepts, steps and optimizations required for efficient AI inference on mobile NPUs. Next, we will go into more detail about the latest mobile platforms from Qualcomm, MediaTek, Google, Samsung, Unisoc and Apple released during the past year, and will compare their inference speed when running common computer vision models. We will talk about power efficiency of mobile NPUs, and will analyze their energy consumption during typical AI workloads. Finally, we will go beyond Android and iOS, covering the topics of AI model deployment on NPUs in Windows, Linux and MacOS systems, analyzing available ML frameworks and their performance. </p>
		</div>

	</div>
	
	<div class="talk-invited">

		<div class="block-1">
			<img style="cursor: pointer; margin-bottom: 0.4vw;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="">
			<img style="cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Chia-Ming.jpg">
			<img style="cursor: pointer; margin-top: 0.4vw;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Yu-Syuan.jpg">
			<img style="cursor: pointer; margin-top: 0.4vw;" onClick='document.getElementById("bio-allen").style.display="inline-block";' src="assets/speakers/Hao-Yun.jpg">
			<a href="https://www.mediatek.com/" target="_blank"><img style="margin-top: 0.7vw; width: 80%" src="assets/organizers/mediatek.png"></a>
		</div>
		
		<div class="block-2">
			<p style="font-weight: 600; font-size: 1.0vw;">Recent Mobile AI Advances and Case Studies</p></br>
			<p style="font-size: 0.9vw;"> <span class="timing">09:00 Pacific Time</span> &nbsp; ┈ &nbsp; <span style="color: rgb(234, 29, 111); cursor: pointer;" onClick='document.getElementById("bio-allen").style.display="inline-block";'>CM Cheng, YuSyuan Xu and Haoyun Chen</span> &nbsp; ┈ &nbsp; <span style="font-style: italic;">MediaTek Inc.</span></p></br>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw; display: none;" id="bio-allen"> <span style="font-weight: 600;">Biography: Dr. Chia-Ming Cheng is a Senior Manager at MediaTek’s Computing and Artificial Intelligence Technology Group. He is currently responsible for the technical planning and management of AI technologies across a range of products, including smartphones, AR/VR devices, smart IoT solutions, and smart TVs. Dr. Cheng also leads initiatives in advanced AI technology through industry-academia collaborations and ecosystem partnerships. He has extensive experience in the research, design, and mass production of computational photography technologies for smartphones. Dr. Cheng holds both Master and Ph.D. degree in Computer Science from National Tsing-Hua University. His research interests primarily focus on computer vision, machine learning, computational photography, 3D modeling and rendering.</br></br>
			
			Yu-Syuan Xu is an Senior AI engineer at MediaTek. His research mainly focuses on deep learning and its applications, specifically on developing and deploying the latest and trendiest AI models on edge devices equipped with MediaTek APU. He is responsible for co-designing AI accelerators from an algorithmic point of view. He received both his B.S. degree and M.S. degree in computer science from National Tsing Hua University in 2017 and 2018, respectively, under the guidance of Prof. Chun-Yi Lee. You can learn more about his work on his website at https://xusean0118.github.io/ and on his Google Scholar profile <a href="https://scholar.google.com/citations?user=QA3VSaYAAAAJ" target="_blank">here</a>. </br></br>
			
			Hao-Yun Chen is an Senior AI engineer at MediaTek. His research mainly focuses on deep learning and its applications, specifically on developing and deploying the latest and trendiest LLM models on edge devices equipped with MediaTek APU. He is responsible for developing LLM long context technique to be compatible with Mediatek platform, with performance and quality optimization. He received both his B.S. degree and M.S. degree in computer science from National Tsing Hua University in 2018 and 2020, respectively, under the guidance of Prof. Shih-Chieh Chang. You can learn more about his work on on his Google Scholar profile <a href="https://scholar.google.com/citations?user=zbhpHsYAAAAJ&hl=zh-TW" target="_blank">here</a>.  </br></br>
			
			</p>
			
			<p style="font-size: 0.9vw; text-align: justify; line-height: 1.2vw; letter-spacing: 0.008vw;"> <span style="font-weight: 600;">Abstract:</span> In this talk, MediaTek will provide you with an overview of their software and hardware platforms and several mobile-oriented research topics. The first part of the talk would be devoted to the discussion of the recent mobile AI advances and MediaTek AI ecosystem. The second part will focus on two recent case studies: Vision Mamba and MOE, which are related to efficient model deployment of mobile AI hardware.
 </p>
		</div>

	</div>
	
	
	<div class="talk-challenges">
		<a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Ignatov_Quantized_Image_Super-Resolution_on_Mobile_NPUs_Mobile_AI_2025_Challenge_CVPRW_2025_paper.pdf" target="_blank">
			<div class="challenge">
				<img src="assets/challenges/title_superres.jpg">
				<p class="title"> Quantized Image Super-Resolution on Mobile NPUs:&nbsp; Results and Top Solutions <p>
				<p class="presenter"> <span class="timing">09:40 Pacific Time</span> &nbsp; ┈ &nbsp; Andrey Ignatov, ETH Zurich <p>	
				<div class="details">
					<div class="target">
						<table>
							<tr>
								<td style="width: 45%;"><img style="height: 1.2vw; float: right;" src="assets/img/soc.png"></td>
								<td style="width: 55%; text-align: left; padding-left: 0.0vw;"><span class="grey">Evaluation Platform:</span> <br> Google Tensor NPU </td>
							</tr>
						</table>
					</div>
				</div>
			</div>
		</a>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">09:50 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;">RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution</span> </span></p></br>
			<p class="authors">Biao Wu, Diankai Zhang, Shaoli Liu, Si Gao, Chengjian Zheng, Ning Wang</p></br>
			<p class="affiliation">☉ &nbsp;ZTE Corporation</p></br>
		</div>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">10:05 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Li_CDVS_Compressed_Domain_On_Device_Memory_Efficient_8K_Video_SlowMo_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">CDVS: Compressed Domain On Device Memory Efficient 8K Video SlowMo</a></span> </span></p></br>
			<p class="authors">Jing Li, Chengyu Wang, Hamid Sheikh, SeokJun Lee</p></br>
			<p class="affiliation">☉ &nbsp;Samsung Research America & Purdue University</p></br>
		</div>
	</div>
	
	<div class="talk-challenges">
		<a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Ignatov_Learned_Smartphone_ISP_on_Mobile_GPUs_Mobile_AI_2025_Challenge_CVPRW_2025_paper.pdf" target="_blank">
			<div class="challenge">
				<img src="assets/challenges/title_raw.jpg">
				<p class="title"> Learned Smartphone ISP on Mobile GPUs Challenge:&nbsp; Results and Top Solutions <p>
				<p class="presenter"> <span class="timing">10:20 Pacific Time</span> &nbsp; ┈ &nbsp; Andrey Ignatov, ETH Zurich <p>	
				<div class="details">
					<div class="target">
						<table>
							<tr>
								<td style="width: 35%;"><img style="height: 1.2vw; float: right;" src="assets/img/soc.png"></td>
								<td style="width: 65%; text-align: left; padding-left: 0.0vw;"><span class="grey">Evaluation Platform:</span> <br> Snapdragon 8 Elite (Adreno GPU) / Dimensity 9400 (Mali GPU) </td>
							</tr>
						</table>
					</div>
				</div>
			</div>
		</a>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">10:30 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Arhire_Learned_Lightweight_Smartphone_ISP_with_Unpaired_Data_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">Learned Lightweight Smartphone ISP with Unpaired Data</a></span> </span></p></br>
			<p class="authors">Andrei Arhire, Radu Timofte</p></br>
			<p class="affiliation">☉ &nbsp;Alexandru Ioan Cuza University</p></br>
		</div>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">10:45 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Wang_Compressed_Domain_Multiframe_Processing_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">Compressed Domain Multiframe Processing</a></span> </span></p></br>
			<p class="authors">Chengyu Wang, Jing Li, Saurabh Kumar, Seok-Jun Lee, Hamid Sheikh</p></br>
			<p class="affiliation">☉ &nbsp;Samsung Research America & SRI-Bangalore</p></br>
		</div>
	</div>
	
	<div class="accepted-paper" style="margin-top: 4vw; margin-bottom: 4vw;">
	<hr></br>
		<div class="block-1"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">11:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;">Break & Lunch</span> </span></p></br></br>
		</div>
	<hr>
	</div>
	
	<div class="talk-challenges">
		<a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Ignatov_RGB_Photo_Enhancement_on_Mobile_GPUs_Mobile_AI_2025_Challenge_CVPRW_2025_paper.pdf" target="_blank">
			<div class="challenge">
				<img src="assets/challenges/title_rgb.jpg">
				<p class="title"> RGB Photo Enhancement on Mobile GPUs Challenge:&nbsp; Results and Top Solutions <p>
				<p class="presenter"> <span class="timing">12:00 Pacific Time</span> &nbsp; ┈ &nbsp; Andrey Ignatov, ETH Zurich <p>	
				<div class="details">
					<div class="target">
						<table>
							<tr>
								<td style="width: 35%;"><img style="height: 1.2vw; float: right;" src="assets/img/soc.png"></td>
								<td style="width: 65%; text-align: left; padding-left: 0.0vw;"><span class="grey">Evaluation Platform:</span> <br> Snapdragon 8 Elite (Adreno GPU) / Dimensity 9400 (Mali GPU) </td>
							</tr>
						</table>
					</div>
				</div>
			</div>
		</a>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">12:15 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Augustin_PETAH_Parameter_Efficient_Task_Adaptation_for_Hybrid_Transformers_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers</a></span> </span></p></br>
			<p class="authors">Syed Shakib Sarwar, Mostafa Elhoushi, Maximilian Augusting, Yuecheng Li, Sai Zhang, Barbara De Salvo</p></br>
			<p class="affiliation">☉ &nbsp;Meta Inc. & IEEE & University of Tubingen & New York University</p></br>
		</div>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">12:30 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Sah_ActNAS__Generating_Efficient_YOLO_Models_using_Activation_NAS_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">ActNAS: Generating Efficient YOLO Models using Activation NAS</a></span> </span></p></br>
			<p class="authors">Sudhakar Sah, Ravish Kumar, Darshan Ganji, Ehsan Saboori</p></br>
			<p class="affiliation">☉ &nbsp;Deeplite</p></br>
		</div>
	</div>

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">12:45 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Thoma_FLAR-SVD_Fast_and_Latency-Aware_Singular_Value_Decomposition_for_Model_Compression_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">FLAR-SVD: Fast and Latency-Aware Singular Value Decomposition for Model Compression</a></span> </span></p></br>
			<p class="authors">Moritz Thoma, Jorge Villasante, Emad Aghajanzadeh, Shambhavi Sampath, Pierpaolo Mori et al.</p></br>
			<p class="affiliation">☉ &nbsp;BMW Group & Technical University of Munich</p></br>
		</div>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">13:15 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Huang_Robust_6DoF_Pose_Estimation_Against_Depth_Noise_and_a_Comprehensive_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">Robust 6DoF Pose Estimation Against Depth Noise and Evaluation on a Mobile Dataset</a></span> </span></p></br>
			<p class="authors">Zixun Huang, Keling Yao, Zhihao Zhao, Chuanyu Pan, Allen Yang</p></br>
			<p class="affiliation">☉ &nbsp;UC Berkeley & Carnegie Mellon University & University of California</p></br>
		</div>
	</div>	

	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">13:30 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Phan-Nguyen_Cycle_Training_with_Semi-Supervised_Domain_Adaptation_Bridging_Accuracy_and_Efficiency_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">Cycle Training with Semi-Supervised Domain Adaptation for Real-Time Mobile Scene Detection</a></span> </span></p></br>
			<p class="authors">Huu-Phong Phan-Nguyen, Anh Dao, Tien-Huy Nguyen, Tuan Quang et al.</p></br>
			<p class="affiliation">☉ &nbsp;University of Infomation Technology & Michigan State University & LPL Financial Corp et al.</p></br>
		</div>
	</div>
	
	<div class="accepted-paper">
		<div class="block-1"><img class="paper-icon" src="assets/img/paper_icon.png"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">13:45 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"><a href="https://openaccess.thecvf.com/content/CVPR2025W/MAI/papers/Sampath_RepFC_Universal_Structural_Reparametrization_Block_for_High_Performance_Lightweight_Deep_CVPRW_2025_paper.pdf" target="_blank" class="link_red_dotted">RepFC: Universal Structural Reparametrization Block for High Performance</a></span> </span></p></br>
			<p class="authors">Shambhavi Balamuthu Sampath, Judeson Anthony Fernando, Moritz Thoma, Nael Fasfous et al.</p></br>
			<p class="affiliation">☉ &nbsp;BMW Group & Technical University of Munich</p></br>
		</div>
	</div>	
		
	</br>
	<hr>
	</br>
	</br>
	
	<div class="accepted-paper" >
		<div class="block-1"></div>			
		<div class="block-2">
			<p class="paper_title"><span class="timing">14:00 Pacific Time</span> &nbsp; &nbsp;
			<span style="font-weight: bold;"> Discussion & Wrap Up </span> </span></p></br>
		</div>
	</div>
	
</div-->

<div id="challenges">
	<p class="title"> PREVIOUS CHALLENGES &nbsp;(2022) </p>
		
	<a href="https://polybox.ethz.ch/index.php/s/t6raVPNRmpKxfVU" target="_blank">
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1756" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> MediaTek Dimensity APU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/mediatek.png"></td></tr></table>
			</div>				
		</div>
	</a>

	<a href="https://polybox.ethz.ch/index.php/s/PilVu1xvj9Kmmfz" target="_blank">		
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1755" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Synaptics Dolphin NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/synaptics.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://polybox.ethz.ch/index.php/s/CxqzyzdBn0AgSZc" target="_blank">		
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1759" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/oppo.png"></td></tr></table>
			</div>				

		</div>
	</a>

	<a href="https://polybox.ethz.ch/index.php/s/jt8sTpOcP8SmgBd" target="_blank">
	<!--a href="https://codalab.lisn.upsaclay.fr/competitions/1760" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Bokeh Effect Rendering <p>		
			<img src="assets/challenges/title_bokeh.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Arm Mali GPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/huawei.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://polybox.ethz.ch/index.php/s/1UM1O9RviLi5vMM" target="_blank">
	<!--a href="https://platform.difficu.lt" target="_blank"-->
		<div class="challenge">
			
			<p class="title"> Depth Estimation <p>		
			<img src="assets/challenges/title_depth.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 4</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/raspberry.png"></td></tr></table>
			</div>				
		</div>
	</a>

</div>
	
<div id="challenges">
	<p class="title"> PREVIOUS CHALLENGES &nbsp;(2021) </p>

	<a href="https://arxiv.org/pdf/2105.07809.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28054 -->
		<div class="challenge">
			
			<p class="title"> Learned Smartphone ISP <p>		
			<img src="assets/challenges/title_raw.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> MediaTek Dimensity APU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/mediatek.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.08629.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28120 -->
		<div class="challenge">
			
			<p class="title"> Image Denoising <p>		
			<img src="assets/challenges/title_denoising.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Exynos Mali GPU </td></tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/samsung.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.07825.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28119 -->
		<div class="challenge">
			
			<p class="title"> Image Super-Resolution <p>		
			<img src="assets/challenges/title_superres.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Synaptics Dolphin NPU</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/synaptics.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://arxiv.org/pdf/2105.08826.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28112 -->
		<div class="challenge">
			
			<p class="title"> Video Super-Resolution <p>		
			<img src="assets/challenges/title_video.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Snapdragon Adreno GPU </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/oppo.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
	<a href="https://arxiv.org/pdf/2105.08630.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28122 -->
		<div class="challenge">
			
			<p class="title"> Depth Estimation <p>		
			<img src="assets/challenges/title_depth.jpg">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Raspberry Pi 4</td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/raspberry.png"></td></tr></table>
			</div>				
		</div>
	</a>
		
	<a href="https://arxiv.org/pdf/2105.08819.pdf" target="_blank">
		<!-- Codalab link: https://competitions.codalab.org/competitions/28113 -->
		<div class="challenge">
			
			<p class="title"> Camera Scene Detection <p>		
			<img src="assets/challenges/title_scene.png">

			<div class="details">
				<div class="target">
					<table> <tr> <td><img src="assets/img/soc.png"></td><td><span class="grey">Evaluation Platform:</span> <br> Apple Bionic </td>	</tr></table>
				</div>			
				<hr>
				<table><tr><td>Powered by:</td><td><img src="assets/organizers/cvl.png"></td></tr></table>
			</div>				
		</div>
	</a>
	
</div>

<!--div id="organizers">
	<p class="title"> ORGANIZERS </p>	
	
	<p class="tba">[ TO BE ANNOUNCED ]</p>

	<!--<img style="width: 100%;" src="assets/organizers/organizers.png"></td><td><span class="grey">

</div-->

<div id="papers">
	<p class="title"> CALL FOR PAPERS </p>
	
	<p class="description">
	Being a part of <span style="font-weight: bold;">CVPR 2026,</span> we invite the authors to submit high-quality original papers proposing various machine learning based solutions for mobile, embedded and IoT platforms. The topics of interest cover all major aspects of AI and deep learning research for mobile devices including, but not limited to:

	</p>
	
	<table>
	
		<tr>
			<th style="width: 50%;"></th>
			<th style="width: 50%;"></th>
		</tr>
	
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Efficient deep learning models for mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Image / video super-resolution on low-power hardware</p></td>
		</tr>
				<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Efficient LLM architectures for mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Optimized Stable Diffusion for mobile devices</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; General smartphone photo and video enhancement</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Deep learning applications for mobile camera ISPs</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Fast image classification / object detection algorithms</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Real-time semantic image segmentation</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Image or sensor based identity recognition</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Activity recognition using smartphone sensors</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Depth estimation w/o multiple cameras</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Portrait segmentation / bokeh effect rendering</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Perceptual image manipulation on mobile devices</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; NLP models optimized for mobile inference</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Artifacts removal from mobile photos / videos</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; RAW image and video processing</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; Low-power machine learning inference</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Machine and deep learning frameworks for mobile devices</p></td>
		</tr>
		<tr>
			<td><p class="topic">&bull;&nbsp;&nbsp; AI performance evaluation of mobile and IoT hardware</p></td>
			<td><p class="topic">&bull;&nbsp;&nbsp; Industry-driven applications related to the above problems</p></td>
		</tr>
		
	</table>
	
	<p class="description">
	To ensure high quality of the accepted papers, all submissions will be evaluated by research and industry experts from the corresponding fields.
	All accepted workshop papers will be published in the <span style="font-weight: bold;">CVPR 2026 Workshop Proceedings</span> by <a href="https://openaccess.thecvf.com/CVPR2020_workshops/CVPR2020_w31" target="_blank" style="font-style: italic; text-decoration-style: dotted; color: rgb(23, 142, 210);">Computer Vision Foundation Open Access</a> and <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" style="font-style: italic; text-decoration-style: dotted; color: rgb(23, 142, 210);">IEEE Xplore Digital Library.</a>
	The authors of the best selected papers will be invited to present their work during the actual workshop event at CVPR 2026.
	</br></br>
	The detailed submission instructions and guidelines can be found <a href="#submission" style="text-decoration: none; color: rgb(23, 142, 210);">here.</a>
	</p>
	
</div>

<div id="submission">

	<p class="title"> SUBMISSION DETAILS @ CVPR </p>
	
	<table class="timeline" style="margin-top: 4vw;">

	 	<tbody>

	        <tr><td style="width: 26%;">Format and paper length </td>
	        	<td class="submission_details">A paper submission has to be in English, in pdf format, and at most 8 pages (excluding references) in double column. The paper format must follow the same guidelines as for all CVPR 2026 submissions: <a href="https://cvpr.thecvf.com/Conferences/2026/AuthorGuidelines" target="_blank" class="link_red_dotted">https://cvpr.thecvf.com/Conferences/2026/AuthorGuidelines</a> </td></tr>

			<tr><td>Author kit </td>
	        	<td class="submission_details">The author kit provides a LaTeX2e template for paper submissions. Please refer to this kit for detailed formatting instructions: <a href="https://github.com/cvpr-org/author-kit/archive/refs/tags/CVPR2026-v1(latex).zip" class="link_red_dotted">https://github.com/cvpr-org/author-kit/archive/refs/tags/CVPR2026-v1(latex).zip</a>
	        	 </td></tr>

	        <tr><td>Double-blind review policy </td>
	        	<td class="submission_details">The review process is double blind. Authors do not know the names of the chair / reviewers of their papers. Reviewers do not know the names of the authors. </td></tr>
	        	
	        <tr><td>Dual submission policy </td>
	        	<td class="submission_details">Dual submission is allowed with CVPR 2026 main conference only. If a paper is submitted also to CVPR and accepted, the paper cannot be published both at the CVPR and the workshop.
	        	 </td></tr>

			<tr><td>Proceedings </td>
	        	<td class="submission_details">Accepted and presented papers will be published after the conference in CVPR Workshops proceedings together with the CVPR 2026 main conference papers.
	        	 </td></tr>

			<tr><td>Submission site * </td>
	        	<td class="submission_details">
	        	<a href="https://cmt3.research.microsoft.com/MAI2026" target="_blank" class="link_red_dotted">https://cmt3.research.microsoft.com/MAI2026</a>
	        	 </td></tr>
	        	 
	        <tr><td></td><td style="text-align: left; font-size: 0.67vw;">* The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.</td></tr>

	    </tbody>
	    	    
	</table>
	
</div>

<div id="dates">
	<p class="title"> TIMELINE</p>
	
	<table class="timeline">
	    <thead>
	        <tr>
	            <th style="width: 64%;">Workshop Event</th>
	            <th>Date &nbsp; [ Pacific Time, 2026 ]</th>
	        </tr>
	    </thead>
	 	<tbody>
	        <tr><td>Website online</td><td>January 20</td></tr>
	        <tr><td>Paper submission server online </td><td> February 9 </td></tr>
	        <tr><td>Paper submission deadline [early submission] </td><td><span style="color: rgb(234, 29, 111);">March 10</span></td></tr>
			<tr><td>Paper decision notification [early submission] </td><td>March 31</td></tr>
	        <tr><td>Paper submission deadline [late submission & challenge papers] </td><td><span style="color: rgb(234, 29, 111);">March 24</span></td></tr>
			<tr><td>Paper decision notification [late submission] </td><td>March 31</td></tr>
	        <tr><td>Camera ready deadline </td><td>April 10</td></tr>
	       	<tr><td>Workshop day </td><td><span style="color: rgb(234, 29, 111);">June (TBA)</span></td></tr>
	    </tbody>
	</table>
	
	<table class="timeline">
	    <thead>
	        <tr>
	            <th style="width: 64%;">Challenges</th>
	            <th>Date &nbsp; [ Pacific Time, 2026 ]</th>
	        </tr>
	    </thead>
	 	<tbody>
	        <tr><td>Website online</td><td>January 15</td></tr>
	        <tr><td>Validation server online </td><td> February 1 </td></tr>
	        <tr><td>Test phase begins, test data released </td><td><span style="color: rgb(234, 29, 111);">March 10</span></td></tr>
			<tr><td>Test phase submission deadline </td><td>March 17</td></tr>
	        <tr><td>Fact sheets, code/executable submission deadline </td><td><span style="color: rgb(234, 29, 111);">March 17</span></td></tr>
			<tr><td>Preliminary test results release to the participants </td><td>March 19</td></tr>
	       	<tr><td>Paper submission deadline for entries from the challenges </td><td><span style="color: rgb(234, 29, 111);">March 24</span></td></tr>
	    </tbody>
	</table>
	
</div>

<div id="tutorial">
	<p class="title"> DEEP LEARNING ON MOBILE DEVICES: TUTORIAL </p>
	
	<iframe style="width: 50vw; height: 28.125vw; margin-top: 5vw;" src="https://www.youtube.com/embed/hUC7SGEYsl0?si=T9Ch137afyJ_DzJn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	
	<iframe style="width: 50vw; height: 28.125vw; margin-top: 3.4vw;" src="https://www.youtube.com/embed/tdpmvy2Cab0?start=795" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>></iframe>
		
	<iframe style="width: 50vw; height: 28.125vw; margin-top: 3.4vw;" src="https://www.youtube.com/embed/aaM_iZrW9Y4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	
	<p style="margin-top: 2.4vw; font-size: 0.98vw;"> Have some questions?&nbsp; Leave them on the <a href="https://ai-benchmark.net/" target="_blank" style="color: rgba(23, 142, 210, 1.0); text-decoration-style: dotted;">AI Benchmark Forum</a></p>
		
</div>

<div id="runtime">
	<p class="title"> RUNTIME VALIDATION </p>
	<p class="description">
	In each MAI 2026 challenge track, the participants have a possibility to check the runtime of their solutions remotely on the target platforms. For this, the converted <span style="font-style: italic;">TensorFlow Lite</span> models should be uploaded to a <span style="font-weight: bold;">special web-server</span>, and their runtime on the actual target devices will be returned instantaneously or withing 24 hours, depending on the track. The detailed model conversion instructions and links can be found in the corresponding challenges.
	</br>
	</br>
	Besides that, we strongly encourage the participants to check the speed and RAM consumption of the obtained models locally on your own Android devices. This will allow you to perform model profiling and debugging faster and much more efficiently. To do this, one can use <a href="http://ai-benchmark.com/" target="_blank" class="link_red_dotted">AI Benchmark application</a> allowing you to load a custom TFLite model and run it with various acceleration options, including <span style="font-style: italic;">CPU</span>, <span style="font-style: italic;">GPU</span>, <span style="font-style: italic;">DSP</span> and <span style="font-style: italic;">NPU</span>:
	</br>
	</br>
	<span class="instruction_item">1.&nbsp; Download AI Benchmark from the <a href="https://play.google.com/store/apps/details?id=org.benchmark.demo" target="_blank" class="link_red_dotted">Google Play</a> / <a href="https://ai-benchmark.com/download.html" target="_blank" class="link_red_dotted">website</a> and run its standard tests.</span></br>
	<span class="instruction_item">2.&nbsp; After the end of the tests, enter the <span style="font-weight: bold;">PRO Mode</span> and select the <span style="font-weight: bold;">Custom Model</span> tab there.</span></br>
	<span class="instruction_item">3.&nbsp; Rename the exported TFLite model to <span style="font-style: italic;">model.tflite</span> and put it into the <span style="font-weight: bold;">Download</span> folder of your device.</span></br>
	<span class="instruction_item">4.&nbsp; Select your mode type, the desired acceleration / inference options and run the model.</span></br>
	</br>
	You can find the screenshots demonstrating these 4 steps below:</br>
	</p>
	<img src="assets/img/ai_benchmark_custom.png">
	
</div>

<div id="contacts">
	<p class="title">CONTACTS</p>
	
	<table>
		<tr>
			
			<td><img style="width: 10vw; vertical-align: top; margin-left: -0.6vw; border-radius: 0.32vw;" src="assets/img/andrey.jpg"></td>
			
			<td style="text-align: left; vertical-align: top; text-align: left; padding-left: 0.9vw;">
				<p style="margin-top: 1.2vw;"><a target="_blank" href="https://www.linkedin.com/in/andrey-ignatov" style="color: #ea1d6f; font-size: 1.2vw; text-decoration: none;">
					Andrey Ignatov
				</a></p>
				<p style="margin-top: 0.7vw;">Computer Vision Lab </p>
				<p>ETH Zurich, Switzerland </p>
				<p style="margin-top: 0.5vw; font-size: 0.9vw; color: #ea1d6f;">andrey@vision.ee.ethz.ch</p>
			</td>
			
			<td><img style="width: 10vw; vertical-align: top; margin-left: 2.6vw; border-radius: 0.32vw;" src="assets/img/radu.jpg"></td>
			
			<td style="text-align: left; vertical-align: top; text-align:left; padding-left: 0.5vw;">
				<p style="margin-top: 1.2vw;"><a target="_blank" href="https://www.informatik.uni-wuerzburg.de/computervision/home/" style="color: #ea1d6f; font-size: 1.2vw; text-decoration: none;">
					Radu Timofte
				</a></p>
				<p style="margin-top: 0.7vw;">Computer Vision Laboratory </p>
				<p>University of Würzburg, Germany </p>
				<p style="margin-top: 0.5vw; font-size: 0.9vw; color: #ea1d6f;">radu.timofte@uni-wuerzburg.de</p>
			</td>
						
		</tr>
	</table>
	
</div>

<div id="about">
	<div id="content">
		<p> ETH Zurich</p>
		<p style="color: #ea1d6f;">Switzerland, 2026</p>
	</div>
</div>

<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script>
<script src="js/main.js"></script>

</body>

</html>
