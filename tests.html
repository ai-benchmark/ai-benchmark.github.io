<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en">

<head>
    <meta charset="utf-8">
    <title> AI-Benchmark </title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,700italic,400italic" rel="stylesheet" type="text/css">
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <script src="js/modernizr.js"></script>
    <meta name="viewport" content="width=device-width">

	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118781498-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-118781498-1');
	</script>
    
</head>

<body data-spy="scroll" data-offset="0" data-target="#navbar-main" style="background-image: url(assets/img/2L_texture_blank.jpg);">

<nav>
	<ul class="left_bar">
		<a href="index.html"> <img src="assets/img/splash.png"> </a>
	</ul>
	<ul class="middle_bar">
	
		<a href="index.html"> <li> BENCHMARK </li></a>
		<a href="ranking.html"> <li> RANKING </li></a>
		<a href="news.html"> <li style="background-color: rgba(233,79,112, 0.85); margin-right: -0.25vw; margin-left: -0.25vw;"> NEWS </li></a>
		<a href="#tests"> <li> AI-TESTS </li></a>

	</ul>
		<ul class="right_bar">
		<a href="download.html"> <img style="width: 7.5vw; margin-top: 1.07vw; position: fixed; margin-left: -9.5vw;" src="assets/img/download.png"> </a>
		<a href="research.html"> <li style="color: #e94f70; font-size: 0.8vw; font-weight: 600;"> RESEARCH &nbsp; <img src="assets/img/goto.png"> </li></a>
		<a href="about.html" style="font-size: 1.0vw;"> <li> ABOUT </li></a>
	</ul>
</nav>

<div id="nav_mobile">
	<ul class="middle_bar">
		<a href="index.html"> <li> BENCHMARK </li></a>
		<a href="news.html"> <li> NEWS </li></a>
		<a href="ranking.html"> <li> RANKING </li></a>
		<a href="tests.html"> <li> AI-TESTS </li></a>
		<a href="research.html" > <li style="color: #e94f70; font-size: 1.57vw; font-weight: 600; margin-left: 1.2vw;"> RESEARCH &nbsp; <img style="width: 1.2vw;" src="assets/img/goto.png"> </li></a>
		<a href="download.html"> <img style="width: 11.2vw; padding-left: 3.6vw; vertical-align: middle;" src="assets/img/download.png"> </a>
	</ul>
</div>

<div id="tests">

<p class="abstract">The benchmark consists of <span style="color: #ea1d6f;">78 AI and Computer Vision tests</span> performed by neural networks running on your smartphone. It measures <span style="color: #178ed2;">over 180 different aspects</span> of AI performance, including the speed, accuracy, initialization time, etc. Considered neural networks comprise a comprehensive range of architectures allowing to assess the performance and limits of various approaches used to solve different AI tasks. A detailed description of the <span style="color: #ea1d6f;">26 benchmark sections</span> is provided below.</p>

<!------------------------------ Task 1 ----------------------------->

<div class="task">

	<img class="img_style_left_1" src="assets/img/screens/v5/screen_1.png">
	
	<p class="title_class_1"><span class="title_class_task">Section 1:</span> &nbsp; <span class="title_class_name">Object Recognition / Classification 
	</p>
	<p class="desc_class_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">MobileNet - V2 &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">224 x 224 px</p>
	<p class="desc_class_2"><span class="desc_class_name">Accuracy on ImageNet:</span> &nbsp; <span class="desc_class_value">71.9 %</p>
	<p class="desc_class_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1801.04381.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py" target="_blank">code</a></p>

	<p class="desc_class_4"> A very small yet already powerful neural network that is able to recognize 1000 different object classes based on a single photo with an accuracy of ~72%. After quantization, its size is less than 4Mb, which together with its low RAM consumption allows to lanch it on absolutely any currently existing smartphone.</p>

</div>

<!------------------------------ Task 2 ----------------------------->

<div class="task">

	<img class="img_style_right_1" src="assets/img/screens/v5/screen_2.png">
	
	<p class="title_class_l_1"><span class="title_class_task">Section 2:</span> &nbsp; <span class="title_class_name">Object Recognition / Classification </p>
	<p class="desc_class_l_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">Inception - V3 &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">346 x 346 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Accuracy on ImageNet:</span> &nbsp; <span class="desc_class_value">78.0 %</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v3.py" target="_blank">code</a></p>

	<p class="desc_class_l_4"> A different approach for the same task: now significantly more accurate, but at the expense of 6x larger size and increased computational requirements. As a clear bonus &mdash; can process images of higher resolutions, which allows for more accurate recognition and smaller object detection.</p>

</div>

<!------------------------------ Task 3 ----------------------------->

<div class="task">

	<img class="img_style_left_2" src="assets/img/screens/v5/screen_3.png">
	
	<p class="title_class_1"><span class="title_class_task">Section 3:</span> &nbsp; <span class="title_class_name">Face Recognition </p>
	<p class="desc_class_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">MobileNet - V3 &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">512 x 512 px</p>
	<p class="desc_class_2"><span class="desc_class_name">Accuracy on ImageNet:</span> &nbsp; <span class="desc_class_value">75.2 %</p>
	<p class="desc_class_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py" target="_blank">code</a></p>

	<p class="desc_class_4_2"> This task probably doesn't need an introduction: based on the face photo you want to identify the person. This is done in the following way: for each face image, a neural network produces a small feature vector that encodes the face and is invariant to its scaling, shifts and rotations. Then this vector is used to retrieve the most similar vector (and the respective identity) from your database that contains the same information about hundreds or millions of people. </p>

</div>

<!------------------------------ Task 4 ----------------------------->

<div class="task">

	<img class="img_style_right_1" src="assets/img/screens/v5/screen_4.png">
	
	<p class="title_class_l_1"><span class="title_class_task">Section 4:</span> &nbsp; <span class="title_class_name">Camera Scene Detection </p>
	<p class="desc_class_l_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">EfficientNet-B4 &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">380 x 380 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Accuracy on ImageNet:</span> &nbsp; <span class="desc_class_value">82.9 %</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/2105.07869.pdf" target="_blank">paper</a>, <a href="https://arxiv.org/pdf/1905.11946.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" target="_blank">code</a></p>

	<p class="desc_class_l_4"> Almost every mobile camera app has now a dedicated "AI mode" analyzing and recognizing the scene that you are capturing using neural networks. In this section, the camera scene detection task is performed using the latest state-of-the-art EfficientNet-B4 model that is able to recognize 30 different photo categories including <span style="font-style: italic;">group portrait, landscape, macro, underwater, food, indoor, stage, fireworks, documents, snow,</span> etc.</p>

</div>

<!------------------------------ Task 5 ----------------------------->

<div class="task">

	<img class="img_style_left_2" src="assets/img/screens/v5/screen_5.png">
	
	<p class="title_class_1"><span class="title_class_task">Sections 5-6:</span> &nbsp; <span class="title_class_name">Parallel Object Recognition </p>
	<p class="desc_class_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">Inception - V3 &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">346 x 346 px</p>
	<p class="desc_class_2"><span class="desc_class_name">Accuracy on ImageNet:</span> &nbsp; <span class="desc_class_value">78.0 %</p>
	<p class="desc_class_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py" target="_blank">code</a></p>

	<p class="desc_class_4_2"> What happens when several programs try to run their AI models at the same time on your device? Will it be able to accelerate all of them? To answer this question, we are running <span style="font-weight: bold;">up to 8</span> floating-point and quantized Inception-V3 neural networks <span style="font-weight: bold;">in parallel</span> on your phone's NPU and DSP, measuring the resulting inference time for each AI model and analyzing the latencies obtained for each setup.</p>

</div>

<!------------------------------ Task 7 ----------------------------->

<div class="task">

	<img class="img_style_right_1" src="assets/img/screens/v5/screen_7.png">
	
	<p class="title_class_l_1"><span class="title_class_task">Section 7:</span> &nbsp; <span class="title_class_name">Object Detection / Tracking </p>
	<p class="desc_class_l_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">YOLOv4-Tiny &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">416 x 416 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Accuracy on MS COCO (mAP):</span> &nbsp; <span class="desc_class_value">40.2</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/2004.10934.pdf" target="_blank">paper</a> / <a href="https://github.com/hunglc007/tensorflow-yolov4-tflite" target="_blank">code</a></p>

	<p class="desc_class_l_4"> It's good to recognize what objects are on the image, but would be even better to detect their <span style="font-weight: bold;">precise location</span> and being able to <span style="font-weight: bold;">track</span> their movements when dealing with video data! For this, one needs a different model trained to perform object tracking. YOLO-V4 is one of the latest architectures available for this task that can detect and track 80 different object categories in real time on mobile devices.
 </p>

</div>

<!------------------------------ Task 8 ----------------------------->

<div class="task">

	<img class="img_style_left_1" src="assets/img/screens/v4/screen_5.png">
	
	<p class="title_class_1"><span class="title_class_task">Section 8:</span> &nbsp; <span class="title_class_name">Optical Character Recognition 
	</p>
	<p class="desc_class_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">CRNN / Bi-LSTM &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16&nbsp; + &nbsp;FP32</p>
	<p class="desc_class_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">64 x 200 px</p>
	<p class="desc_class_2"><span class="desc_class_name">IC13 Score:</span> &nbsp; <span class="desc_class_value">86.7 %</p>
	<p class="desc_class_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1507.05717.pdf" target="_blank">paper</a> / <a href="https://github.com/Belval/CRNN" target="_blank">code</a></p>

	<p class="desc_class_4"> A very standard task performed by a very standard end-to-end trained LSTM-based CRNN model. This neural network consists of two parts: the first one is a well-knows <span style="font-weight: bold;">ResNet-34</span> network that is used here to generate deep features for the input data, while the second one, <span style="font-weight: bold;">Bidirectional Static RNN</span>, is taking these features as an input and predicts the actual words / letters on the image.</p>

</div>

<!------------------------------ Tasks 9-10 ----------------------------->

<div class="task">

	<figure class="cd-image-container img_style_right_2">
		<img src="assets/img/screens/v4/screen_10a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		
		<div class="cd-resize-img">
			<img src="assets/img/screens/v4/screen_10b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Segmented</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>
	
	
	<p class="title_class_l_1"><span class="title_class_task">Sections 9-10:</span> &nbsp; <span class="title_class_name">Semantic Segmentation </p>
	<p class="desc_class_l_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">DeepLab-V3+ &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1024 x 1024 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">CityScapes (mIoU):</span> &nbsp; <span class="desc_class_value">82.1 %</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank">code</a></p>
	<p class="desc_class_l_4_2"> Running Self-Driving algorithm on your phone? Yes, that's possible too, at least you can perform a substantial part of this task &mdash; detect 19 categories of objects (e.g. car, pedestrian, road, sky, etc.) based on the photo from the camera mounted inside the car. On the right image, one can see the results of such pixel-size semantic segmentation (each color correpsonds to each object class) for a very popular DeepLab-V3+ network designed specifically for low-power devices. </p>

</div>

<!------------------------------ Task 11 ----------------------------->

<div class="task">

	<figure class="cd-image-container img_style_right_2" >
		<img src="assets/img/screens/v4/screen_6a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Blurred</span>
		
		<div class="cd-resize-img">
			<img src="assets/img/screens/v4/screen_6b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Restored</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>
	
	
	<p class="title_class_l_1"><span class="title_class_task">Section 11:</span> &nbsp; <span class="title_class_name">Image Deblurring </p>
	<p class="desc_class_l_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">IMDN &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1024 x 1024 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Set-5 Score (x3):</span> &nbsp; <span class="desc_class_value">34.36 dB</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1909.11856.pdf" target="_blank">paper</a> / <a href="https://github.com/Zheng222/IMDN" target="_blank">code</a></p>
	<p class="desc_class_l_4_2"> Remember taking blurry photos using your phone camera? So, this is the task: make them sharp again. In the simplest case, this kind of distortions is modeled by applying a Gaussian blur to normal uncorrupted images, and then trying to restore them back with a neural network. In this task, the blur is removed by a popular albeit computationally demanding IMDN model (based on the IMDB blocks) that shows nice results on many image restoration problems. </p>

</div>

<!------------------------------ Task 12 ----------------------------->

<div class="task">

	<figure class="cd-image-container image_style_3">
		<img src="assets/img/screens/v4/screen_7a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		<div class="cd-resize-img">
			<img src="assets/img/screens/v4/screen_7b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Restored</span>
		</div>
		<span class="cd-handle cd-first"></span>
	</figure>
	
	<p class="title_class_1_3"><span class="title_class_task">Section 12:</span> &nbsp; <span class="title_class_name">Image Super-Resolution</p>
	<p class="desc_class_1_3"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">ESRGAN &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">512 x 512 px</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Set-5 Score (x4):</span> &nbsp; <span class="desc_class_value">32.73 dB</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1809.00219.pdf" target="_blank">paper</a> / <a href="https://github.com/peteryuX/esrgan-tf2" target="_blank">code</a></p>
	<p class="desc_class_4_3"> Have you ever zoomed your photos? Remember artifacts, lack of details and sharpness? Then you know this task from your own experience: make zoomed photos look as good as the original images. In this case, the network is trained to do an equivalent task: to restore the original photo given its downscaled (e.g., by factor of 4) version. Here we consider a very powerful ESRGAN model reconstructing photo-realistic images even when dealing with pictures of very low resolution. Besides the standard loss functions, ESRGAN also uses a bit of magic called the GAN loss...</p>

</div>

<!------------------------------ Task 13 ----------------------------->

<div class="task">

	<figure class="cd-image-container img_style_right_2" >
		<img src="assets/img/screens/v4/screen_8a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		
		<div class="cd-resize-img">
			<img src="assets/img/screens/v4/screen_8b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Restored</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>
	
	
	<p class="title_class_l_1"><span class="title_class_task">Section 13:</span> &nbsp; <span class="title_class_name">Image Super-Resolution </p>
	<p class="desc_class_l_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">SRGAN &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1024 x 1024 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Set-5 Score (x4):</span> &nbsp; <span class="desc_class_value">29.40 dB</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1609.04802.pdf" target="_blank">paper</a> / <a href="https://github.com/tensorlayer/srgan" target="_blank">code</a></p>
	<p class="desc_class_l_4_2"> What if we train our neural network using... another neural network? Yes, two networks performing two tasks: network A is trying to solve our super-resolution problem, while network B observes its results, tries to find some flaws there and then penalizes the network A accordingly. Sounds cool? In fact, it is cool: for the SR task, this approach called the <span style="font-weight: bold;">GAN training</span> was first introduced in the SRGAN paper. While it has its own issues, the produced results are often looking really amazing. </p>

</div>

<!------------------------------ Task 14 ----------------------------->

<div class="task">

	<figure class="cd-image-container image_style_3">
		<img src="assets/img/screens/v5/screen_14a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		<div class="cd-resize-img">
			<img src="assets/img/screens/v5/screen_14b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Restored</span>
		</div>
		<span class="cd-handle cd-first"></span>
	</figure>
	
	<p class="title_class_1_3"><span class="title_class_task">Section 14:</span> &nbsp; <span class="title_class_name">Image Denoising </p>
	<p class="desc_class_1_3"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">U-Net &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1024 x 1024 px</p>
	<p class="desc_class_2_3"><span class="desc_class_name">ISBI (IoU):</span> &nbsp; <span class="desc_class_value">0.9203</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank">paper</a> / <a href="https://github.com/zhixuhao/unet/blob/master/model.py" target="_blank">code</a></p>
	<p class="desc_class_4_3"> Noise is arguably the number one issue affecting the quality of mobile photos. You think that your latest flagship cameraphone doesn't suffer from it? Just enable the RAW mode in your camera app and capture a few photos in moderate lighting conditions — the amount of noise on the original unprocessed images would likely impress you. Then how to remove it efficiently? For this, one can use a U-Net shaped CNN trained on paired noisy / noise free images: the resulting neural network would be able to fight noise even on photos taken in the most complex lighting scenarios.</p>

</div>

<!------------------------------ Task 15 ----------------------------->

<div class="task">

	<figure class="cd-image-container img_style_right_2" >
		<img src="assets/img/screens/v5/screen_15a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		
		<div class="cd-resize-img">
			<img src="assets/img/screens/v5/screen_15b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Depth</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>
	
	
	<p class="title_class_l_1"><span class="title_class_task">Section 15:</span> &nbsp; <span class="title_class_name">Depth Estimation </p>
	<p class="desc_class_l_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">MV3-Depth &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1024 x 1536 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">ZED dataset (si-RMSE):</span> &nbsp; <span class="desc_class_value">0.2836</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Zhang_A_Simple_Baseline_for_Fast_and_Accurate_Depth_Estimation_on_CVPRW_2021_paper.pdf" target="_blank">paper</a> / <a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_and_Accurate_Single-Image_Depth_Estimation_on_Mobile_Devices_Mobile_CVPRW_2021_paper.pdf" target="_blank">paper</a> </p>
	<p class="desc_class_l_4_2"> Yet another task performed by almost any recent mobile camera application: when selecting the <span style="font-style: italic;">portrait</span> or <span style="font-style: italic;">bokeh</span> modes, it tries to estimate the depth of the scene so that the correct amount of blur is applied to each photo region. While one usually needs the information from multiple cameras or ToF sensors for this, it is also possible to predict the depth quite accurately based on one single image. Moreover, the considered MV3-Depth network can perform this task extremely fast, achieving more than 30 FPS on the recent smartphone chipsets.</p>

</div>


<!------------------------------ Task 16-17 ----------------------------->

<div class="task">

	<figure class="cd-image-container image_style_3">
		<img src="assets/img/screens/v4/screen_11a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		<div class="cd-resize-img">
			<img src="assets/img/screens/v4/screen_11b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Enhanced</span>
		</div>
		<span class="cd-handle cd-first"></span>
	</figure>
	
	<p class="title_class_1_3"><span class="title_class_task">Sections 16-17:</span> &nbsp; <span class="title_class_name">Photo Enhancement</p>
	<p class="desc_class_1_3"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">DPED &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1536 x 2048 / 1024 x 1536 px</p>
	<p class="desc_class_2_3"><span class="desc_class_name">DPED PSNR i-Score:</span> &nbsp; <span class="desc_class_value">18.11 dB</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="http://people.ee.ethz.ch/~ihnatova/index.html" target="_blank">paper</a> / <a href="http://people.ee.ethz.ch/~ihnatova/wespe.html" target="_blank">paper</a> / <a href="https://github.com/aiff22/dped" target="_blank">code</a></p>
	<p class="desc_class_4_3"> Struggling when looking at photos from your old phone? This can be fixed: a properly trained neural network can make photos even from an ancient iPhone 3GS device looking nice and up-to-date. To achieve this, it observes and learns how to transform photos from a low-quality device into the same photos from a high-end DSLR camera. Of course, there are some obvious limitations for this magic (e.g., the network should be retrained for each new phone model), but the resulting images are looking quite good, especially for old devices.</p>

</div>

<!------------------------------ Task 18 ----------------------------->

<div class="task">

	<figure class="cd-image-container img_style_right_2" >
		<img src="assets/img/screens/v4/screen_9a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">Original</span>
		
		<div class="cd-resize-img">
			<img src="assets/img/screens/v4/screen_9b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Bokeh</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>
	
	
	<p class="title_class_l_1"><span class="title_class_task">Section 18:</span> &nbsp; <span class="title_class_name">Bokeh Simulation </p>
	<p class="desc_class_l_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">PyNET+ &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">512 x 1024 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">EBB! PSNR Score:</span> &nbsp; <span class="desc_class_value">23.28 dB</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/2006.05698.pdf" target="_blank">paper</a> / <a href="https://github.com/aiff22/PyNET-Bokeh" target="_blank">code</a></p>
	<p class="desc_class_l_4_2"> We already mentioned this well-known and popular AI task — blurring the background like on high-end DSLRs: just select the <span style="font-style: italic;">portrait</span> mode in the camera app to see how it works on your device. In this section, a powerful PyNET+ model processing each input image in parallel at three different scales and utilizing all available NPU computational resources is used to render bokeh effect without the need of multiple cameras: after being pre-trained, it can add an artistic blur to arbitrary images.</p>

</div>

<!------------------------------ Task 19 ----------------------------->

<div class="task">

	<figure class="cd-image-container image_style_3">
		<img src="assets/img/screens/v5/screen_19a.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">RAW</span>
		<div class="cd-resize-img">
			<img src="assets/img/screens/v5/screen_19b.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Restored</span>
		</div>
		<span class="cd-handle cd-first"></span>
	</figure>
	
	<p class="title_class_1_3"><span class="title_class_task">Section 19:</span> &nbsp; <span class="title_class_name">Learned Smartphone ISP</p>
	<p class="desc_class_1_3"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">PUNET &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1088 x 1920 px</p>
	<p class="desc_class_2_3"><span class="desc_class_name">FujiFilm UltraISP Score:</span> &nbsp; <span class="desc_class_value">23.03 dB</p>
	<p class="desc_class_2_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/2002.05509.pdf" target="_blank">paper</a> / <a href="https://arxiv.org/pdf/2105.07809.pdf" target="_blank">paper</a> / <a href="https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp" target="_blank">code</a></p>
	<p class="desc_class_4_3"> Let's increase the level of insanity: instead of enhancing various aspects of mobile photos separately, we will now replace the entire RAW image processing pipeline with a single neural network trained to do all the tasks at once including <span style="font-style: italic;">photo demosaicing, denoising, deblurring, super-resolution, tone mapping,</span> etc. This approach not only works well, but is able to compete with traditional ISP systems in terms of the resulting photo quality. However, you will need an extremely powerful NPU for this task as fast large-resolution image processing is needed here.  </p>

</div>

<!------------------------------ Task 20 ----------------------------->

<div class="task">

	<img class="img_style_right_1" src="assets/img/screens/v5/screen_20.png">
	
	<p class="title_class_l_1"><span class="title_class_task">Section 20:</span> &nbsp; <span class="title_class_name">Video Super-Resolution </p>
	<p class="desc_class_l_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">XLSR &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">1080 x 1920 px</p>
	<p class="desc_class_l_2"><span class="desc_class_name">DIV2K Score (x3):</span> &nbsp; <span class="desc_class_value">30.11 dB</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ayazoglu_Extremely_Lightweight_Quantization_Robust_Real-Time_Single-Image_Super_Resolution_for_Mobile_CVPRW_2021_paper.pdf" target="_blank">paper</a> / <a href="https://github.com/cxzhou95/XLSR" target="_blank">code</a></p>
	<p class="desc_class_l_4"> We have already seen the image SR task — so, what's new in the video super-resolution problem? Well, the first and the major change is the tightened computational requirements: now we want to super-resolve an even larger-resolution <span style="font-weight: bold;">FullHD</span> videos in real time at more than <span style="font-weight: bold;">30 FPS</span>! Thus, the previously considered SRGAN / ESRGAN models would be too slow here, and we need to use something more efficient and NPU-friendly, like the XLSR neural network. </p>

</div>

<!------------------------------ Task 21-22 ----------------------------->

<div class="task">

	<img class="img_style_left_2" src="assets/img/screens/v5/screen_21.png">
	
	<p class="title_class_1"><span class="title_class_task">Sections 21-22:</span> &nbsp; <span class="title_class_name">Video Super-Resolution </p>
	<p class="desc_class_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">VSR &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">2160 x 3840 px</p>
	<p class="desc_class_2"><span class="desc_class_name">DIV2K Score (x3):</span> &nbsp; <span class="desc_class_value">29.75 dB</p>
	<p class="desc_class_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/2105.07825.pdf" target="_blank">paper</a></p>

	<p class="desc_class_4_2"> We can go even further — as the resolution of many mobile and TV screens already exceeds 2K, we can try to perform 4K video super-resolution bearing in mind our target frame rate of 30 FPS. In this task, we use a very efficient VSR model designed for mobile and IoT devices, requiring very little computational effort: you can technically run this network even on a 10-year old the <span style="font-style: italic;">Samsung Galaxy S2</span> phone, though a latency of 12 seconds per frame would probably upset you... </p>

</div>

<!------------------------------ Task 23 ----------------------------->

<div class="task">

	<img class="img_style_right_1" src="assets/img/screens/v4/screen_13.png">
	
	<p class="title_class_l_1"><span class="title_class_task">Section 23:</span> &nbsp; <span class="title_class_name">Text Completion </p>
	<p class="desc_class_l_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">Static RNN / LSTM &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16&nbsp; + &nbsp;FP32</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Embeddings Size:</span> &nbsp; <span class="desc_class_value">32 x 500</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Layers &nbsp;|&nbsp; LSTM Units:</span> &nbsp; <span class="desc_class_value">4 &nbsp;|&nbsp; 512</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank">paper</a> / <a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py" target="_blank">code</a></p>

	<p class="desc_class_l_4"> Yet another standard deep learning problem on smartphones &mdash; providing text suggestions based on what the user types. In this task, we consider a variation of this NLP task: a simple static LSTM model learns to fill in the gaps in the text using sentence semantics inferred from the provided <span style="font-weight: bold;">Word2vec</span> word embeddings.</p>

</div>

<!------------------------------ Task 25 ----------------------------->

<div class="task">

	<img class="img_style_left_2" src="assets/img/screens/v5/screen_24.png">
	
	<p class="title_class_1"><span class="title_class_task">Sections 24:</span> &nbsp; <span class="title_class_name">Question Answering </p>
	<p class="desc_class_1_2"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">MobileBERT &nbsp; | &nbsp; <span class="mode">INT8</p>
	<p class="desc_class_2"><span class="desc_class_name">Embeddings Size:</span> &nbsp; <span class="desc_class_value">384</p>
	<p class="desc_class_2"><span class="desc_class_name">SQuAD V2.0 Score:</span> &nbsp; <span class="desc_class_value">79.2</p>
	<p class="desc_class_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/2004.02984.pdf" target="_blank">paper</a> / <a href="https://github.com/google-research/bert" target="_blank">code</a></p>

	<p class="desc_class_4_2"> Just 15 years ago, it was hard to imagine that one would be able to talk to a phone and ask it questions about some random things. Was even harder to imagine that the phone would be easily understanding your requests and providing you with the correct answers. Yet, this is the reality — with the MobileBERT model, it is possible to ask this network arbitrary questions, and it would be looking for the answers in the specified text databases. What really impresses is its performance: the responses are very accurate and come almost instantaneously. </p>

</div>

<!------------------------------ Task 24 ----------------------------->

<div class="task">

	<img class="img_style_right_1" src="assets/img/screens/v5/screen_25.png">
	
	<p class="title_class_l_1"><span class="title_class_task">Section 25:</span> &nbsp; <span class="title_class_name">Text Completion </p>
	<p class="desc_class_l_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">ALBERT &nbsp; | &nbsp; <span class="mode">FP16</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Embeddings Size:</span> &nbsp; <span class="desc_class_value">128</p>
	<p class="desc_class_l_2"><span class="desc_class_name">SQuAD V2.0 Score:</span> &nbsp; <span class="desc_class_value">80.0</p>
	<p class="desc_class_l_2"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank">paper</a> / <a href="https://github.com/google-research/albert" target="_blank">code</a></p>
	<p class="desc_class_l_4"> ALBERT is a further extension of the original BERT architecture: being lighter, smaller (just 45 MB for FP32 model) and faster, this model is a good choice for mobile devices. One can use this neural network for many different NLP tasks including question answering, filling in the gaps in the text, intelligent reply suggestions, or even songs completion that can be seen in this test.</p>
</div>


<!------------------------------ Task 26 ----------------------------->

<div class="task">

	<img class="img_style_left_1" src="assets/img/screens/v4/screen_14.png">
	
	<p class="title_class_1"><span class="title_class_task">Section 26:</span> &nbsp; <span class="title_class_name">Memory Limits </p>
	<p class="desc_class_1"><span class="desc_class_name">Neural Network:</span> &nbsp; <span class="desc_class_value">ResNet &nbsp; | &nbsp; <span class="mode">INT8&nbsp; + &nbsp;FP16</p>
	<p class="desc_class_2"><span class="desc_class_name">Image Resolution:</span> &nbsp; <span class="desc_class_value">9 MP</p>
	<p class="desc_class_2"><span class="desc_class_name"># Parameters:</span> &nbsp; <span class="desc_class_value">372.803</p>
	<p class="desc_class_3"><span class="desc_class_name">Paper & Code Links:</span> &nbsp; <span class="desc_class_value"><a href="https://arxiv.org/pdf/1704.02470.pdf" target="_blank">paper</a> / <a href="https://github.com/aiff22/DPED/blob/master/models.py" target="_blank">code</a></p>
	<p class="desc_class_4"> It's not all about the speed — there is a little use of a powerful mobile NPU if it cannot process high-resolution images. In the past sections, we have already seen that one might need neural networks for photo denosing, super-resolution, dehazing, ISP processing or bokeh effect rendering, and in all these tasks we are dealing with at least 8-12MP images. This test is aimed at finding the limits of your device: how big images can it handle when using a common ResNet network?  </p>
</div>

</div>

<div id="about">
	<div id="content">
		<p >Copyright © 2022 by A.I.</p>
		<p style="color: #ea1d6f">ETH Zurich, Switzerland</p>
	</div>
</div>

<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script>
<script src="js/main.js"></script>

</body>

</html>
